{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "66b6eb9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000000, 61)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import requirements \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load dataset\n",
    "data = pd.read_csv('Divar.csv')\n",
    "data.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "854f2515",
   "metadata": {},
   "outputs": [],
   "source": [
    "# outlier handling and cleaning for numeric columns\n",
    "\n",
    "def clean_numeric_columns(df):\n",
    "    df_clean = df.copy()\n",
    "    \n",
    "    # 1. outlier handling for financial columns\n",
    "    financial_cols = ['price_value', 'rent_value', 'credit_value']\n",
    "    \n",
    "    for col in financial_cols:\n",
    "        if col in df_clean.columns:\n",
    "            # zero values to NaN\n",
    "            zero_mask = df_clean[col] == 0\n",
    "            if zero_mask.sum() > 0:\n",
    "                print(f\"{col}: {zero_mask.sum()} zero values -> NaN\")\n",
    "                df_clean.loc[zero_mask, col] = np.nan\n",
    "            \n",
    "            # Winsorization for outliers\n",
    "            lower_limit = df_clean[col].quantile(0.01)\n",
    "            upper_limit = df_clean[col].quantile(0.99)\n",
    "            \n",
    "            outliers_mask = (df_clean[col] < lower_limit) | (df_clean[col] > upper_limit)\n",
    "            if outliers_mask.sum() > 0:\n",
    "                print(f\"{col}: {outliers_mask.sum()} outlier -> winsorize\")\n",
    "                df_clean[col] = np.clip(df_clean[col], lower_limit, upper_limit)\n",
    "    \n",
    "    return df_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e23c2693",
   "metadata": {},
   "outputs": [],
   "source": [
    "# specific column conversions\n",
    "\n",
    "def transform_special_columns(df):\n",
    "    df_transformed = df.copy()\n",
    "    \n",
    "    # 1. rooms_count convert to numeric\n",
    "    if 'rooms_count' in df_transformed.columns:\n",
    "        persian_to_number = {\n",
    "            'یک': 1, 'دو': 2, 'سه': 3, 'چهار': 4,\n",
    "            'پنج': 5, 'شش': 6, 'هفت': 7, 'هشت': 8,\n",
    "            'نه': 9, 'ده': 10, 'بدون اتاق': 0,\n",
    "            'پنج یا بیشتر': 5\n",
    "        }\n",
    "        \n",
    "        # translate Persian words to numbers\n",
    "        df_transformed['rooms_count'] = df_transformed['rooms_count'].map(persian_to_number)\n",
    "        \n",
    "        # convert to numeric\n",
    "        df_transformed['rooms_count'] = pd.to_numeric(df_transformed['rooms_count'], errors='coerce')\n",
    "    \n",
    "    # 2. construction_year conversion\n",
    "    if 'construction_year' in df_transformed.columns:\n",
    "        def extract_year(x):\n",
    "            if pd.isna(x):\n",
    "                return np.nan\n",
    "            x = str(x)\n",
    "            # extract digits\n",
    "            numbers = []\n",
    "            for char in x:\n",
    "                if char.isdigit():\n",
    "                    numbers.append(char)\n",
    "                elif char in ['۰', '۱', '۲', '۳', '۴', '۵', '۶', '۷', '۸', '۹']:\n",
    "                    persian_digits = {'۰':'0', '۱':'1', '۲':'2', '۳':'3', '۴':'4',\n",
    "                                    '۵':'5', '۶':'6', '۷':'7', '۸':'8', '۹':'9'}\n",
    "                    numbers.append(persian_digits[char])\n",
    "            \n",
    "            if numbers:\n",
    "                year_str = ''.join(numbers[-4:])  # 4 last digits\n",
    "                if len(year_str) == 4:\n",
    "                    year = int(year_str)\n",
    "                    # convert Gregorian to Hijri if needed\n",
    "                    if 1300 <= year <= 1500:  # hijri range\n",
    "                        return year\n",
    "            return np.nan\n",
    "        \n",
    "        df_transformed['construction_year'] = df_transformed['construction_year'].apply(extract_year)\n",
    "    \n",
    "    return df_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9a68c3c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature engineering for clustering\n",
    "\n",
    "def create_clustering_features(df):\n",
    "    df_features = df.copy()\n",
    "    \n",
    "    # main features for clustering\n",
    "    \n",
    "    # location-based features \n",
    "    if 'city_slug' in df_features.columns:\n",
    "        # Frequency encoding for city_slug\n",
    "        city_freq = df_features['city_slug'].value_counts(normalize=True)\n",
    "        df_features['city_freq_encoded'] = df_features['city_slug'].map(city_freq)\n",
    "    \n",
    "    # physical features\n",
    "    # building size and number of rooms are important\n",
    "    physical_features = []\n",
    "    if 'building_size' in df_features.columns:\n",
    "        physical_features.append('building_size')\n",
    "        # make bins for building size\n",
    "        df_features['size_category'] = pd.cut(df_features['building_size'], \n",
    "                                            bins=[0, 50, 100, 150, 200, 500, np.inf],\n",
    "                                            labels=['x-small', 'small', 'medium', 'large', 'x-large', 'huge'])\n",
    "    \n",
    "    if 'rooms_count' in df_features.columns:\n",
    "        physical_features.append('rooms_count')\n",
    "        # categorize number of rooms\n",
    "        df_features['rooms_category'] = pd.cut(df_features['rooms_count'].fillna(0),\n",
    "                                             bins=[-1, 0, 1, 2, 3, 4, np.inf],\n",
    "                                             labels=['no room', 'one room', 'two rooms', 'three rooms', 'four rooms', 'five rooms+'])\n",
    "\n",
    "    # amenities (sum of boolean amenities)\n",
    "    boolean_cols = ['has_elevator', 'has_parking', 'has_warehouse', 'has_balcony',\n",
    "                   'has_water', 'has_electricity', 'has_gas']\n",
    "    \n",
    "    existing_bools = [col for col in boolean_cols if col in df_features.columns]\n",
    "    if existing_bools:\n",
    "        # covert boolean strings to 1/0\n",
    "        for col in existing_bools:\n",
    "            df_features[col] = df_features[col].map({'true': 1, 'True': 1, True: 1,\n",
    "                                                    'false': 0, 'False': 0, False: 0,\n",
    "                                                    np.nan: 0})\n",
    "        \n",
    "        df_features['amenities_count'] = df_features[existing_bools].sum(axis=1)\n",
    "    \n",
    "    # d) property type (from cat2_slug)\n",
    "    if 'cat2_slug' in df_features.columns:\n",
    "        # One-hot encoding or label encoding\n",
    "        property_type_mapping = {\n",
    "            'residential-sell': 0,\n",
    "            'residential-rent': 1,\n",
    "            'commercial-rent': 2,\n",
    "            'commercial-sell': 3,\n",
    "            'temporary-rent': 4,\n",
    "            'real-estate-services': 5\n",
    "        }\n",
    "        df_features['property_type_encoded'] = df_features['cat2_slug'].map(property_type_mapping)\n",
    "    \n",
    "    return df_features, physical_features + ['city_freq_encoded', 'amenities_count', 'property_type_encoded']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "da9d1523",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fiture engineering for prediction\n",
    "def create_prediction_features(df):\n",
    "    df_features = df.copy()\n",
    "    \n",
    "    # 1. Target Variable definition\n",
    "    # so far, we have multiple targets:\n",
    "    # - (price_value)\n",
    "    # - (rent_value)  \n",
    "    # - (credit_value)\n",
    "\n",
    "    # 2. important features for prediction price:\n",
    "    prediction_features = []\n",
    "    \n",
    "    # geographical features\n",
    "    if all(col in df_features.columns for col in ['location_latitude', 'location_longitude']):\n",
    "        # make distance from city center\n",
    "        # for tehran we assume center coordinates\n",
    "        tehran_center = (35.6892, 51.3890)\n",
    "        df_features['distance_from_center'] = haversine_distance(\n",
    "            df_features['location_latitude'], df_features['location_longitude'],\n",
    "            tehran_center[0], tehran_center[1]\n",
    "        )\n",
    "        prediction_features.append('distance_from_center')\n",
    "    \n",
    "    # physical features with interactions\n",
    "    if 'building_size' in df_features.columns:\n",
    "        prediction_features.append('building_size')\n",
    "        \n",
    "        # price per sqm (if price_value exists)\n",
    "        if 'price_value' in df_features.columns:\n",
    "            df_features['price_per_sqm'] = df_features['price_value'] / df_features['building_size']\n",
    "            # log transform for reducing skewness\n",
    "            df_features['log_price_per_sqm'] = np.log1p(df_features['price_per_sqm'])\n",
    "    \n",
    "    if 'rooms_count' in df_features.columns:\n",
    "        prediction_features.append('rooms_count')\n",
    "        \n",
    "        # room density\n",
    "        if 'building_size' in df_features.columns:\n",
    "            df_features['room_density'] = df_features['rooms_count'] / df_features['building_size']\n",
    "            prediction_features.append('room_density')\n",
    "    \n",
    "    # property age\n",
    "    if 'construction_year' in df_features.columns:\n",
    "        current_year = 1404 # current year in Hijri\n",
    "        df_features['property_age'] = current_year - df_features['construction_year']\n",
    "        prediction_features.append('property_age')\n",
    "        \n",
    "        # is new property (age <= 5 years)\n",
    "        df_features['is_new_property'] = (df_features['property_age'] <= 5).astype(int)\n",
    "        prediction_features.append('is_new_property')\n",
    "    \n",
    "    # amenities (weighted score)\n",
    "    amenity_weights = {\n",
    "        'has_elevator': 0.3,\n",
    "        'has_parking': 0.25,\n",
    "        'has_warehouse': 0.1,\n",
    "        'has_balcony': 0.15,\n",
    "        'has_pool': 0.35,  # luxury amenities\n",
    "        'has_jacuzzi': 0.35,\n",
    "        'has_sauna': 0.35\n",
    "    }\n",
    "    \n",
    "    amenities_score = 0\n",
    "    for amenity, weight in amenity_weights.items():\n",
    "        if amenity in df_features.columns:\n",
    "            df_features[amenity] = df_features[amenity].map({'true': 1, 'True': 1, True: 1,\n",
    "                                                            'false': 0, 'False': 0, False: 0,\n",
    "                                                            np.nan: 0})\n",
    "            amenities_score += df_features[amenity] * weight\n",
    "    \n",
    "    df_features['amenities_score'] = amenities_score\n",
    "    prediction_features.append('amenities_score')\n",
    "    \n",
    "    # user type feature\n",
    "    if 'user_type' in df_features.columns:\n",
    "        # agent vs individual\n",
    "        df_features['is_agent'] = df_features['user_type'].apply(\n",
    "            lambda x: 1 if x == 'مشاور املاک' else 0\n",
    "        )\n",
    "        prediction_features.append('is_agent')\n",
    "    \n",
    "    return df_features, prediction_features\n",
    "\n",
    "def haversine_distance(lat1, lon1, lat2, lon2):\n",
    "    \"\"\"haversine distance calculation\"\"\"\n",
    "    R = 6371  # earth radius in kilometers\n",
    "    \n",
    "    lat1, lon1, lat2, lon2 = map(np.radians, [lat1, lon1, lat2, lon2])\n",
    "    \n",
    "    dlat = lat2 - lat1\n",
    "    dlon = lon2 - lon1\n",
    "    \n",
    "    a = np.sin(dlat/2)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2)**2\n",
    "    c = 2 * np.arcsin(np.sqrt(a))\n",
    "    \n",
    "    return R * c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c7af56b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding categorical features\n",
    "\n",
    "def encode_categorical_columns(df, method='onehot'):\n",
    "    \"\"\"\n",
    "    deferent ways for encoding categorical columns:\n",
    "    - onehot: one-hot encoding for low cardinality columns\n",
    "    - frequency: frequency encoding for high cardinality columns\n",
    "    - target: target encoding (to be implemented with cross-validation)\n",
    "    \"\"\"\n",
    "    df_encoded = df.copy()\n",
    "    \n",
    "    # detecting categorical columns\n",
    "    categorical_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
    "    \n",
    "    # dropping text columns from categorical\n",
    "    text_cols = ['description', 'title']\n",
    "    categorical_cols = [col for col in categorical_cols if col not in text_cols]\n",
    "    \n",
    "    if method == 'onehot':\n",
    "        # One-hot encoding for low cardinality columns\n",
    "        for col in categorical_cols:\n",
    "            if df[col].nunique() <= 10:  # 10 unique values threshold\n",
    "                dummies = pd.get_dummies(df[col], prefix=col, drop_first=True)\n",
    "                df_encoded = pd.concat([df_encoded, dummies], axis=1)\n",
    "                df_encoded = df_encoded.drop(col, axis=1)\n",
    "    \n",
    "    elif method == 'frequency':\n",
    "        # Frequency encoding for high cardinality columns\n",
    "        for col in categorical_cols:\n",
    "            if df[col].nunique() > 10:\n",
    "                freq = df[col].value_counts(normalize=True)\n",
    "                df_encoded[f'{col}_freq'] = df[col].map(freq)\n",
    "                df_encoded = df_encoded.drop(col, axis=1)\n",
    "    \n",
    "    elif method == 'target':\n",
    "        # Target encoding (to be implemented with cross-validation)\n",
    "        pass\n",
    "    \n",
    "    return df_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "6b54d77c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# handling missing values\n",
    "def strategic_missing_value_handling(df):\n",
    "    \"\"\"\n",
    "    missing values management strategy:\n",
    "    1. drop columns with more than 80% missing\n",
    "    2. numeric columns: impute with median\n",
    "    3. categorical columns: impute with mode\n",
    "    \"\"\"\n",
    "    df_clean = df.copy()\n",
    "    \n",
    "    # 1. drop columns with more than 60% missing\n",
    "    missing_threshold = 0.6\n",
    "    high_missing_cols = df_clean.columns[df_clean.isnull().mean() > missing_threshold].tolist()\n",
    "    print(f\"drop columns with more than {missing_threshold*100}% missing: {len(high_missing_cols)} columns\")\n",
    "    df_clean = df_clean.drop(columns=high_missing_cols)\n",
    "    \n",
    "    # 2. numeric columns - impute with median (except target)\n",
    "    numeric_cols = df_clean.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    target_cols = ['price_value', 'rent_value', 'credit_value']\n",
    "    numeric_for_impute = [col for col in numeric_cols if col not in target_cols]\n",
    "    \n",
    "    for col in numeric_for_impute:\n",
    "        if df_clean[col].isnull().sum() > 0:\n",
    "            median_val = df_clean[col].median()\n",
    "            df_clean[col] = df_clean[col].fillna(median_val)\n",
    "    \n",
    "    # 3. categorical columns - impute with mode\n",
    "    categorical_cols = df_clean.select_dtypes(include=['object']).columns.tolist()\n",
    "    for col in categorical_cols:\n",
    "        if df_clean[col].isnull().sum() > 0:\n",
    "            mode_val = df_clean[col].mode()[0] if not df_clean[col].mode().empty else 'Unknown'\n",
    "            df_clean[col] = df_clean[col].fillna(mode_val)\n",
    "    \n",
    "    return df_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "21c3928f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing pipeline class\n",
    "class RealEstatePreprocessor:\n",
    "    \"\"\"preprocessing pipeline for real estate data\"\"\"\n",
    "    \n",
    "    def __init__(self, clustering_features=None, prediction_features=None):\n",
    "        self.clustering_features = clustering_features or []\n",
    "        self.prediction_features = prediction_features or []\n",
    "        self.scalers = {}\n",
    "        self.encoders = {}\n",
    "    \n",
    "    def fit_transform(self, df, task='clustering'):\n",
    "        \"\"\"preprocessing data bases on task\"\"\"\n",
    "        df_processed = df.copy()\n",
    "        \n",
    "        # 1. initial cleaning\n",
    "        df_processed = clean_numeric_columns(df_processed)\n",
    "        df_processed = transform_special_columns(df_processed)\n",
    "        \n",
    "        # 2. missing values management\n",
    "        df_processed = strategic_missing_value_handling(df_processed)\n",
    "        \n",
    "        # 3. Feature Engineering\n",
    "        if task == 'clustering':\n",
    "            df_processed, features = create_clustering_features(df_processed)\n",
    "            self.clustering_features = features\n",
    "        \n",
    "        elif task == 'prediction':\n",
    "            df_processed, features = create_prediction_features(df_processed)\n",
    "            self.prediction_features = features\n",
    "            \n",
    "            # for prediction، just row with available target\n",
    "            target_cols = [col for col in ['price_value', 'rent_value', 'credit_value'] \n",
    "                          if col in df_processed.columns]\n",
    "            if target_cols:\n",
    "                # drop rows with all target missing\n",
    "                mask = df_processed[target_cols].isnull().all(axis=1)\n",
    "                df_processed = df_processed[~mask]\n",
    "        \n",
    "        # 4. Encoding\n",
    "        df_processed = encode_categorical_columns(df_processed, method='frequency')\n",
    "        \n",
    "        # 5. Scaling (only for clustering)\n",
    "        if task == 'clustering':\n",
    "            from sklearn.preprocessing import StandardScaler\n",
    "            scaler = StandardScaler()\n",
    "            features_to_scale = [col for col in self.clustering_features if col in df_processed.columns]\n",
    "            if features_to_scale:\n",
    "                df_processed[features_to_scale] = scaler.fit_transform(df_processed[features_to_scale])\n",
    "                self.scalers['clustering'] = scaler\n",
    "        \n",
    "        return df_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b0aed33f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "price_value: 1902 zero values -> NaN\n",
      "price_value: 10826 outlier -> winsorize\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1000000, 23)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# drop Unnamed: 0 column\n",
    "# data = data.drop('Unnamed: 0', axis=1)\n",
    "\n",
    "# drop columns with high missing values\n",
    "missing_threshold = 0.6\n",
    "cols_to_drop = data.columns[data.isnull().mean() > missing_threshold]\n",
    "data = data.drop(columns=cols_to_drop)\n",
    "\n",
    "# winsorize financial columns\n",
    "df_clean = data.copy()\n",
    "    \n",
    "# 1. outlier handling for financial columns\n",
    "financial_cols = ['price_value', 'rent_value', 'credit_value']\n",
    "\n",
    "for col in financial_cols:\n",
    "    if col in df_clean.columns:\n",
    "        # zero values to NaN\n",
    "        zero_mask = df_clean[col] == 0\n",
    "        if zero_mask.sum() > 0:\n",
    "            print(f\"{col}: {zero_mask.sum()} zero values -> NaN\")\n",
    "            df_clean.loc[zero_mask, col] = np.nan\n",
    "        \n",
    "        # Winsorization for outliers\n",
    "        lower_limit = df_clean[col].quantile(0.01)\n",
    "        upper_limit = df_clean[col].quantile(0.99)\n",
    "        \n",
    "        outliers_mask = (df_clean[col] < lower_limit) | (df_clean[col] > upper_limit)\n",
    "        if outliers_mask.sum() > 0:\n",
    "            print(f\"{col}: {outliers_mask.sum()} outlier -> winsorize\")\n",
    "            df_clean[col] = np.clip(df_clean[col], lower_limit, upper_limit)\n",
    "\n",
    "data = df_clean.copy()\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "8bf7e4ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "price_value: 5665 outlier -> winsorize\n",
      "drop columns with more than 60.0% missing: 0 columns\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(566444, 26)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# preprocessing pipeline instantiation\n",
    "## Sample data for faster processing\n",
    "# sample_data = data.sample(n=100000, random_state=41)\n",
    "\n",
    "# Preprocess for clustering\n",
    "preprocessor = RealEstatePreprocessor()\n",
    "clustering_data = preprocessor.fit_transform(data, task='clustering')\n",
    "# drop discription & title columns if exist\n",
    "clustering_data = clustering_data.drop(columns=[col for col in ['description', 'title'] if col in clustering_data.columns])\n",
    "# drop rows with price_value missing\n",
    "clustering_data = clustering_data[clustering_data['price_value'].notnull()]\n",
    "\n",
    "# select clustering features\n",
    "features = preprocessor.clustering_features\n",
    "X_cluster = clustering_data[features]\n",
    "\n",
    "# K-means clustering\n",
    "from sklearn.cluster import KMeans\n",
    "kmeans = KMeans(n_clusters=10, random_state=42, n_init=10)\n",
    "clusters = kmeans.fit_predict(X_cluster)\n",
    "\n",
    "\n",
    "clustering_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "06da1ac8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "price_value: 5665 outlier -> winsorize\n",
      "drop columns with more than 60.0% missing: 0 columns\n"
     ]
    }
   ],
   "source": [
    "# only rows with price_value\n",
    "prediction_data = data[data['price_value'].notnull()].copy()\n",
    "\n",
    "# Preprocess\n",
    "preprocessor = RealEstatePreprocessor()\n",
    "processed_data = preprocessor.fit_transform(prediction_data, task='prediction')\n",
    "\n",
    "# train-test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X = processed_data[preprocessor.prediction_features]\n",
    "y = processed_data['price_value']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "8143b9d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 566444 entries, 1 to 999997\n",
      "Data columns (total 26 columns):\n",
      " #   Column                  Non-Null Count   Dtype   \n",
      "---  ------                  --------------   -----   \n",
      " 0   Unnamed: 0              566444 non-null  int64   \n",
      " 1   cat2_slug               566444 non-null  object  \n",
      " 2   price_mode              566444 non-null  object  \n",
      " 3   price_value             566444 non-null  float64 \n",
      " 4   building_size           566444 non-null  float64 \n",
      " 5   rooms_count             566444 non-null  float64 \n",
      " 6   has_balcony             566340 non-null  float64 \n",
      " 7   has_elevator            566444 non-null  int64   \n",
      " 8   has_warehouse           566444 non-null  int64   \n",
      " 9   has_parking             566444 non-null  int64   \n",
      " 10  construction_year       566444 non-null  float64 \n",
      " 11  is_rebuilt              566444 non-null  bool    \n",
      " 12  has_restroom            566444 non-null  object  \n",
      " 13  floor_material          566444 non-null  object  \n",
      " 14  location_latitude       566444 non-null  float64 \n",
      " 15  location_longitude      566444 non-null  float64 \n",
      " 16  city_freq_encoded       566444 non-null  float64 \n",
      " 17  size_category           566444 non-null  category\n",
      " 18  rooms_category          566444 non-null  category\n",
      " 19  amenities_count         566444 non-null  float64 \n",
      " 20  property_type_encoded   566444 non-null  float64 \n",
      " 21  cat3_slug_freq          566444 non-null  float64 \n",
      " 22  city_slug_freq          566444 non-null  float64 \n",
      " 23  neighborhood_slug_freq  566444 non-null  float64 \n",
      " 24  created_at_month_freq   566444 non-null  float64 \n",
      " 25  floor_freq              566444 non-null  float64 \n",
      "dtypes: bool(1), category(2), float64(15), int64(4), object(4)\n",
      "memory usage: 105.3+ MB\n"
     ]
    }
   ],
   "source": [
    "clustering_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "420496d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output to csv\n",
    "clustering_data.to_csv('clustering_data.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9337c96c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
