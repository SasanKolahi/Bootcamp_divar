{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "777d285b",
   "metadata": {},
   "source": [
    "# import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7733fd04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from pyproj import Transformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3455d66b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv(\"Divar.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11616d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c6ba86f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['transformable_price'].value_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4de693c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data.loc[\n",
    "    data['construction_year'].astype(str).str.contains('Ù‚Ø¨Ù„ Ø§Ø²', na=False),\n",
    "    'construction_year'\n",
    "] = 1369\n",
    "\n",
    "#created_at_month to dateTime\n",
    "data['created_at_month']=pd.to_datetime(data['created_at_month'],errors='coerce')\n",
    "data['year_month'] = data['created_at_month'].dt.to_period('M')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e753c10a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def persian_to_english_numbers(text):\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "\n",
    "    persian_digits = 'Û°Û±Û²Û³Û´ÛµÛ¶Û·Û¸Û¹'\n",
    "    arabic_digits  = 'Ù Ù¡Ù¢Ù£Ù¤Ù¥Ù¦Ù§Ù¨Ù©'\n",
    "    english_digits = '0123456789'\n",
    "\n",
    "    translation_table = str.maketrans(\n",
    "        persian_digits + arabic_digits,\n",
    "        english_digits * 2\n",
    "    )\n",
    "\n",
    "    return text.translate(translation_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e4a632",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=data.applymap(persian_to_english_numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9fffd73",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['created_at_month']=pd.to_datetime(data['created_at_month'],errors='coerce')\n",
    "data['year_month'] = data['created_at_month'].dt.to_period('M')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f24803",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- 3. ØªØ¨Ø¯ÛŒÙ„ rooms_count Ø¨Ù‡ Ø¹Ø¯Ø¯ ----------\n",
    "def parse_rooms(val):\n",
    "    if pd.isna(val):\n",
    "        return np.nan\n",
    "    s = str(val).strip().lower()\n",
    "    if 'Ø¨Ø¯ÙˆÙ†' in s or 'ØµÙØ±' in s:\n",
    "        return 0\n",
    "    if 'ÛŒÚ©' in s:\n",
    "        return 1\n",
    "    if 'Ø¯Ùˆ' in s:\n",
    "        return 2\n",
    "    if 'Ø³Ù‡' in s:\n",
    "        return 3\n",
    "    if 'Ú†Ù‡Ø§Ø±' in s:\n",
    "        return 4\n",
    "    if 'Ù¾Ù†Ø¬' in s or 'Ø¨ÛŒØ´ØªØ±' in s:\n",
    "        return 5\n",
    "    nums = re.findall(r'\\d+', s)\n",
    "    return int(nums[0]) if nums else np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09cb3654",
   "metadata": {},
   "source": [
    "# Part 1 old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe7ebdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62563e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[['price_value','transformed_rent']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be10be96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------\n",
    "# ğŸ”§ Ø®ÙˆØ´Ù‡â€ŒØ¨Ù†Ø¯ÛŒ Ø§Ù…Ù„Ø§Ú© â€” Ù†Ø³Ø®Ù‡ Ù†Ù‡Ø§ÛŒÛŒ (Ø¨Ø¯ÙˆÙ† outlier)\n",
    "# --------------------------------------------\n",
    "\n",
    "df = data.copy()\n",
    "\n",
    "\n",
    "df['rooms_num'] = df['rooms_count'].apply(parse_rooms).fillna(2)\n",
    "\n",
    "# ---------- 4. Ù…Ø­Ø§Ø³Ø¨Ù‡ transformable_price ----------\n",
    "df['rent_val'] = pd.to_numeric(df['rent_value'], errors='coerce')\n",
    "df['cred_val'] = pd.to_numeric(df['credit_value'], errors='coerce')\n",
    "df['price_val'] = pd.to_numeric(df['price_value'], errors='coerce')\n",
    "\n",
    "def calc_transformable(row):\n",
    "    if 'sell' in str(row['cat2_slug']).lower() and row['price_val'] > 0:\n",
    "        return row['price_val']\n",
    "    if row['cred_val'] > 0 or row['rent_val'] > 0:\n",
    "        return (row['cred_val'] or 0) + (row['rent_val'] or 0) * 1200\n",
    "    return np.nan\n",
    "\n",
    "df['tp'] = df.apply(calc_transformable, axis=1)\n",
    "\n",
    "# ---------- 5. ÙÛŒÙ„ØªØ± Ø§ÙˆÙ„ÛŒÙ‡: ÙÙ‚Ø· Ø±Ú©ÙˆØ±Ø¯Ù‡Ø§ÛŒ Ù…Ø¹ØªØ¨Ø± ----------\n",
    "df = df[\n",
    "    (df['tp'].notna()) & (df['tp'] > 0) &\n",
    "    (df['building_size'].notna()) & (df['building_size'] > 5) &\n",
    "    (df['location_latitude'].notna()) & (df['location_longitude'].notna())\n",
    "].copy()\n",
    "\n",
    "print(f\"âœ… Ù¾Ø³ Ø§Ø² ÙÛŒÙ„ØªØ± Ø§ÙˆÙ„ÛŒÙ‡: {len(df)} Ø±Ú©ÙˆØ±Ø¯\")\n",
    "\n",
    "# ---------- 6. ØªØ¨Ø¯ÛŒÙ„ Ù…Ø®ØªØµØ§Øª Ø¨Ù‡ UTM (Zone 39N) ----------\n",
    "transformer = Transformer.from_crs(\"EPSG:4326\", \"EPSG:32639\")\n",
    "valid_mask = (\n",
    "    df['location_latitude'].between(25, 40) &\n",
    "    df['location_longitude'].between(44, 63)\n",
    ")\n",
    "df = df[valid_mask]\n",
    "\n",
    "lat_utm, lon_utm = transformer.transform(\n",
    "    df['location_latitude'].values,\n",
    "    df['location_longitude'].values\n",
    ")\n",
    "df['lat_utm'] = lat_utm\n",
    "df['lon_utm'] = lon_utm\n",
    "\n",
    "# ---------- 7. Ø­Ø°Ù outlierÙ‡Ø§ÛŒ Ø´Ø¯ÛŒØ¯ (Ø­ÛŒØ§ØªÛŒ!) ----------\n",
    "print(f\"Ù‚Ø¨Ù„ Ø§Ø² ÙÛŒÙ„ØªØ± outlier: max size = {df['building_size'].max():.0f}, max price = {df['tp'].max():.0f}\")\n",
    "\n",
    "# ğŸš« Ø­Ø°Ù:\n",
    "# - Ø²ÛŒØ±Ø¨Ù†Ø§ > 50,000 Ù…ØªØ± (ØºÛŒØ±ÙˆØ§Ù‚Ø¹ÛŒ Ø¨Ø±Ø§ÛŒ Ù…Ø³Ú©ÙˆÙ†ÛŒ)\n",
    "# - Ù‚ÛŒÙ…Øª > 200 Ù…ÛŒÙ„ÛŒØ§Ø±Ø¯ ØªÙˆÙ…Ø§Ù† (Ù¾Ø±Øª Ø´Ø¯ÛŒØ¯)\n",
    "# - Ø²ÛŒØ±Ø¨Ù†Ø§ < 10 Ù…ØªØ± (Ø®Ø±Ø§Ø¨)\n",
    "df_clean = df[\n",
    "    (df['building_size'] > 10) &\n",
    "    (df['building_size'] <= 50_000) &\n",
    "    (df['tp'] <= 200_000_000_000)  # 200 Ù…ÛŒÙ„ÛŒØ§Ø±Ø¯\n",
    "]\n",
    "\n",
    "print(f\"âœ… Ù¾Ø³ Ø§Ø² Ø­Ø°Ù outlier: {len(df_clean)} Ø±Ú©ÙˆØ±Ø¯\")\n",
    "print(f\"   â†’ max size = {df_clean['building_size'].max():.0f}, max price = {df_clean['tp'].max():.0f}\")\n",
    "\n",
    "# ---------- 8. Ø³Ø§Ø®Øª ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ù†Ù‡Ø§ÛŒÛŒ ----------\n",
    "# Ø§Ù…ØªÛŒØ§Ø² Ø§Ù…Ú©Ø§Ù†Ø§Øª (0â€“3)\n",
    "for col in ['has_parking', 'has_elevator', 'has_security_guard']:\n",
    "    df_clean[col] = df_clean[col].astype(str).str.lower().isin(['true', '1', 'yes'])\n",
    "df_clean['amenities'] = (\n",
    "    df_clean['has_parking'].astype(int) +\n",
    "    df_clean['has_elevator'].astype(int) +\n",
    "    df_clean['has_security_guard'].astype(int)\n",
    ")\n",
    "\n",
    "features = [\n",
    "    'tp',               # Ù‚ÛŒÙ…Øª ØªØ¨Ø¯ÛŒÙ„â€ŒØ´Ø¯Ù‡\n",
    "    'building_size',   # Ø²ÛŒØ±Ø¨Ù†Ø§\n",
    "    'rooms_num',       # ØªØ¹Ø¯Ø§Ø¯ Ø§ØªØ§Ù‚\n",
    "    'lat_utm',         # Ù…Ø®ØªØµØ§Øª\n",
    "    'lon_utm',\n",
    "    'amenities'        # Ø§Ù…ØªÛŒØ§Ø² Ø§Ù…Ú©Ø§Ù†Ø§Øª\n",
    "]\n",
    "\n",
    "X = df_clean[features].copy()\n",
    "\n",
    "# ---------- 9. StandardScaler ----------\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# ---------- 10. PCA (90% ÙˆØ§Ø±ÛŒØ§Ù†Ø³) ----------\n",
    "pca = PCA()\n",
    "pca.fit(X_scaled)\n",
    "cumsum = np.cumsum(pca.explained_variance_ratio_)\n",
    "n_comp = np.argmax(cumsum >= 0.90) + 1\n",
    "\n",
    "pca = PCA(n_components=n_comp, random_state=42)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "print(f\"\\nğŸ“Š PCA: {n_comp} Ù…Ø¤Ù„ÙÙ‡ Ø¨Ø±Ø§ÛŒ 90% ÙˆØ§Ø±ÛŒØ§Ù†Ø³ (ÙˆØ§Ø±ÛŒØ§Ù†Ø³: {cumsum[n_comp-1]:.1%})\")\n",
    "print(\"ÙˆØ²Ù†â€ŒÙ‡Ø§ÛŒ Ù…Ø¤Ù„ÙÙ‡ Ø§ÙˆÙ„ (PC1):\")\n",
    "pc1_weights = pd.Series(pca.components_[0], index=features).round(3)\n",
    "print(pc1_weights)\n",
    "\n",
    "# ---------- 11. KMeans (K=10 â€” Ù…Ù†Ø·Ù‚ÛŒ Ø¨Ø±Ø§ÛŒ 2000 Ø±Ú©ÙˆØ±Ø¯) ----------\n",
    "kmeans = KMeans(n_clusters=10, random_state=42, n_init=20)\n",
    "clusters = kmeans.fit_predict(X_pca)\n",
    "df_clean['cluster'] = clusters\n",
    "\n",
    "# ---------- 12. Ø±Ø³Ù… Ù†Ù…ÙˆØ¯Ø§Ø± ----------\n",
    "df_plot = df_clean.copy()\n",
    "df_plot['PC1'] = X_pca[:, 0]\n",
    "df_plot['PC2'] = X_pca[:, 1] if X_pca.shape[1] > 1 else 0\n",
    "df_plot['price_mil'] = df_plot['tp'] / 1_000_000\n",
    "\n",
    "fig = px.scatter(\n",
    "    df_plot,\n",
    "    x='PC1',\n",
    "    y='PC2',\n",
    "    color='cluster',\n",
    "    color_continuous_scale='Rainbow',\n",
    "    labels={'PC1': 'Ù…Ø¤Ù„ÙÙ‡ Ø§ØµÙ„ÛŒ Û±', 'PC2': 'Ù…Ø¤Ù„ÙÙ‡ Ø§ØµÙ„ÛŒ Û²'},\n",
    "    title=f'Ø®ÙˆØ´Ù‡â€ŒØ¨Ù†Ø¯ÛŒ KMeans (K=10) Ø±ÙˆÛŒ ÙØ¶Ø§ÛŒ PCA â€” {len(df_plot)} Ø±Ú©ÙˆØ±Ø¯ Ù¾Ø³ Ø§Ø² Ø­Ø°Ù outlier',\n",
    "    opacity=0.7,\n",
    "    width=950, height=600\n",
    ")\n",
    "\n",
    "# Ù…Ø±Ø§Ú©Ø² Ø®ÙˆØ´Ù‡â€ŒÙ‡Ø§\n",
    "centers_pca = kmeans.cluster_centers_\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=centers_pca[:, 0], \n",
    "    y=centers_pca[:, 1] if centers_pca.shape[1] > 1 else [0]*len(centers_pca),\n",
    "    mode='markers+text',\n",
    "    marker=dict(size=18, color='black', symbol='star', line=dict(width=2, color='white')),\n",
    "    text=[f'C{i}' for i in range(5)],\n",
    "    textposition=\"top center\",\n",
    "    name='Ù…Ø±Ø§Ú©Ø² Ø®ÙˆØ´Ù‡â€ŒÙ‡Ø§'\n",
    "))\n",
    "\n",
    "fig.update_layout(font=dict(size=13), title_font_size=16)\n",
    "fig.write_html(\"kmeans_final.html\")\n",
    "fig.show()\n",
    "\n",
    "# ---------- 13. Ø®Ù„Ø§ØµÙ‡ Ø®ÙˆØ´Ù‡â€ŒÙ‡Ø§ ----------\n",
    "summary = df_plot.groupby('cluster').agg(\n",
    "    count=('tp', 'size'),\n",
    "    avg_price_mil=('price_mil', 'mean'),\n",
    "    avg_size=('building_size', 'mean'),\n",
    "    avg_rooms=('rooms_num', 'mean'),\n",
    "    avg_amenities=('amenities', 'mean'),\n",
    "    main_city=('city_slug', lambda x: x.value_counts().index[0] if len(x.value_counts()) > 0 else 'â€”')\n",
    ").round(2)\n",
    "\n",
    "print(\"\\nğŸ“Š Ø®Ù„Ø§ØµÙ‡ Ù†Ù‡Ø§ÛŒÛŒ Ø®ÙˆØ´Ù‡â€ŒÙ‡Ø§:\")\n",
    "print(summary.sort_values('avg_price_mil', ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a44343ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------\n",
    "# âœ… KMeans Ø¨Ø§ K = 10 â€” Ù…Ø·Ø§Ø¨Ù‚ ØµÙˆØ±Øª Ø³ÙˆØ§Ù„\n",
    "#    Ø¨Ø¯ÙˆÙ† PCAØŒ Ø¨Ø§ Ø­Ø°Ù outlierÙ‡Ø§ Ùˆ scaling ØµØ­ÛŒØ­\n",
    "# --------------------------------------------------\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from pyproj import Transformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "\n",
    "df = data.copy()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df['rooms_num'] = df['rooms_count'].apply(parse_rooms).fillna(2)\n",
    "\n",
    "# ---------- 4. Ù…Ø­Ø§Ø³Ø¨Ù‡ transformable_price ----------\n",
    "# df['rent_val'] = pd.to_numeric(df['rent_value'], errors='coerce')\n",
    "# df['cred_val'] = pd.to_numeric(df['credit_value'], errors='coerce')\n",
    "df['price_val'] = pd.to_numeric(df['price_value'], errors='coerce')\n",
    "\n",
    "# def calc_tp(row):\n",
    "#     # ÙØ±ÙˆØ´\n",
    "#     if 'sell' in str(row['cat2_slug']).lower() and row['price_val'] > 0:\n",
    "#         return row['price_val']\n",
    "#     # Ø±Ù‡Ù† + Ø§Ø¬Ø§Ø±Ù‡\n",
    "#     if row['cred_val'] > 0 or row['rent_val'] > 0:\n",
    "#         return (row['cred_val'] or 0) + (row['rent_val'] or 0) * 1200\n",
    "#     # ÙÙ‚Ø· Ø§Ø¬Ø§Ø±Ù‡\n",
    "#     if row['rent_val'] > 0:\n",
    "#         return row['rent_val'] * 1200\n",
    "#     return np.nan\n",
    "\n",
    "# df['tp'] = df.apply(calc_tp, axis=1)\n",
    "\n",
    "# ---------- 5. ÙÛŒÙ„ØªØ± Ø§ÙˆÙ„ÛŒÙ‡ ----------\n",
    "df = df[\n",
    "    (df['price_val'].notna()) & (df['price_val'] > 10_000_000) &  # â‰¥10 Ù…ÛŒÙ„ÛŒÙˆÙ† ØªÙˆÙ…Ø§Ù†\n",
    "    (df['building_size'].notna()) & (df['building_size'].between(10, 50_000)) &\n",
    "    (df['location_latitude'].between(25, 40)) &\n",
    "    (df['location_longitude'].between(44, 63))\n",
    "].copy()\n",
    "\n",
    "print(f\"âœ… Ù¾Ø³ Ø§Ø² ÙÛŒÙ„ØªØ± Ø§ÙˆÙ„ÛŒÙ‡: {len(df)} Ø±Ú©ÙˆØ±Ø¯\")\n",
    "\n",
    "# ---------- 6. Ø­Ø°Ù outlierÙ‡Ø§ÛŒ Ø´Ø¯ÛŒØ¯ (99.5th percentile) ----------\n",
    "upper_price = df['price_val'].quantile(0.995)   # ~50 Ù…ÛŒÙ„ÛŒØ§Ø±Ø¯\n",
    "upper_size  = df['building_size'].quantile(0.995)  # ~2000 Ù…ØªØ±\n",
    "\n",
    "df_clean = df[\n",
    "    (df['price_val'] <= upper_price) &\n",
    "    (df['building_size'] <= upper_size)\n",
    "]\n",
    "\n",
    "print(f\"âœ… Ù¾Ø³ Ø§Ø² Ø­Ø°Ù outlier: {len(df_clean)} Ø±Ú©ÙˆØ±Ø¯\")\n",
    "\n",
    "# ---------- 7. ØªØ¨Ø¯ÛŒÙ„ Ù…Ø®ØªØµØ§Øª Ø¨Ù‡ UTM (Zone 39N) ----------\n",
    "transformer = Transformer.from_crs(\"EPSG:4326\", \"EPSG:32639\")\n",
    "lat_utm, lon_utm = transformer.transform(\n",
    "    df_clean['location_latitude'].values,\n",
    "    df_clean['location_longitude'].values\n",
    ")\n",
    "df_clean['lat_utm'] = lat_utm\n",
    "df_clean['lon_utm'] = lon_utm\n",
    "\n",
    "# ---------- 8. Ø³Ø§Ø®Øª Ø§Ù…ØªÛŒØ§Ø² Ø§Ù…Ú©Ø§Ù†Ø§Øª ----------\n",
    "for col in ['has_parking', 'has_elevator', 'has_security_guard']:\n",
    "    df_clean[col] = df_clean[col].astype(str).str.lower().isin(['true', '1', 'yes'])\n",
    "df_clean['amenities'] = df_clean[['has_parking', 'has_elevator', 'has_security_guard']].sum(axis=1)\n",
    "\n",
    "# ---------- 9. Ø§Ù†ØªØ®Ø§Ø¨ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ ----------\n",
    "features = [\n",
    "    'price_val',               # Ù‚ÛŒÙ…Øª ØªØ¨Ø¯ÛŒÙ„â€ŒØ´Ø¯Ù‡ (ØªÙˆÙ…Ø§Ù†)\n",
    "    'building_size',   # Ø²ÛŒØ±Ø¨Ù†Ø§ (Ù…ØªØ±)\n",
    "    'rooms_num',       # ØªØ¹Ø¯Ø§Ø¯ Ø§ØªØ§Ù‚\n",
    "    'lat_utm',         # UTM\n",
    "    'lon_utm',\n",
    "    'amenities'        # 0â€“3\n",
    "]\n",
    "\n",
    "X = df_clean[features].copy()\n",
    "\n",
    "# ---------- 10. StandardScaler ----------\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# ---------- 11. âœ… KMeans Ø¨Ø§ Ø¯Ù‚ÛŒÙ‚Ø§Ù‹ K = 10 ----------\n",
    "kmeans = KMeans(n_clusters=10, random_state=42, n_init=25)\n",
    "clusters = kmeans.fit_predict(X_scaled)\n",
    "df_clean['cluster'] = clusters\n",
    "\n",
    "# ---------- 12. Ø±Ø³Ù…: Ù‚ÛŒÙ…Øª vs Ø²ÛŒØ±Ø¨Ù†Ø§ (Ø¨Ø§ 10 Ø®ÙˆØ´Ù‡) ----------\n",
    "df_clean['price_mil'] = df_clean['price_val'] / 1_000_000\n",
    "\n",
    "fig = px.scatter(\n",
    "    df_clean,\n",
    "    x='building_size',\n",
    "    y='price_mil',\n",
    "    color='cluster',\n",
    "    color_continuous_scale='Rainbow',\n",
    "    labels={\n",
    "        'building_size': 'Ø²ÛŒØ±Ø¨Ù†Ø§ (Ù…ØªØ± Ù…Ø±Ø¨Ø¹)',\n",
    "        'price_mil': 'Ù‚ÛŒÙ…Øª ØªØ¨Ø¯ÛŒÙ„â€ŒØ´Ø¯Ù‡ (Ù…ÛŒÙ„ÛŒÙˆÙ† ØªÙˆÙ…Ø§Ù†)',\n",
    "        'cluster': 'Ø®ÙˆØ´Ù‡'\n",
    "    },\n",
    "    title='Ø®ÙˆØ´Ù‡â€ŒØ¨Ù†Ø¯ÛŒ KMeans â€” Ø¯Ù‚ÛŒÙ‚Ø§Ù‹ Û±Û° Ø®ÙˆØ´Ù‡ (Ø¨Ø¯ÙˆÙ† PCA)',\n",
    "    opacity=0.65,\n",
    "    width=950, height=650\n",
    ")\n",
    "\n",
    "# Ù…Ø±Ø§Ú©Ø² Ø®ÙˆØ´Ù‡â€ŒÙ‡Ø§ (Ø¯Ø± ÙØ¶Ø§ÛŒ Ø§ØµÙ„ÛŒ)\n",
    "centers_scaled = kmeans.cluster_centers_\n",
    "centers_original = scaler.inverse_transform(centers_scaled)\n",
    "centers_df = pd.DataFrame(centers_original, columns=features)\n",
    "\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=centers_df['building_size'],\n",
    "    y=centers_df['price_val'] / 1_000_000,\n",
    "    mode='markers+text',\n",
    "    marker=dict(size=18, color='black', symbol='star', line=dict(width=2, color='white')),\n",
    "    text=[f'C{i}' for i in range(10)],\n",
    "    textposition=\"top center\",\n",
    "    name='Ù…Ø±Ø§Ú©Ø² Ø®ÙˆØ´Ù‡â€ŒÙ‡Ø§'\n",
    "))\n",
    "\n",
    "fig.update_layout(font=dict(size=13), title_font_size=18)\n",
    "fig.write_html(\"kmeans_k10_no_pca.html\")\n",
    "fig.show()\n",
    "\n",
    "# ---------- 13. Ø¬Ø¯ÙˆÙ„ Ø®Ù„Ø§ØµÙ‡ (10 Ø®ÙˆØ´Ù‡) ----------\n",
    "summary = df_clean.groupby('cluster').agg(\n",
    "    count=('price_val', 'size'),\n",
    "    avg_price_mil=('price_mil', 'mean'),\n",
    "    avg_size=('building_size', 'mean'),\n",
    "    avg_rooms=('rooms_num', 'mean'),\n",
    "    avg_amenities=('amenities', 'mean'),\n",
    "    main_city=('city_slug', lambda x: x.value_counts().index[0] if len(x.value_counts()) > 0 else 'â€”')\n",
    ").round(2)\n",
    "\n",
    "print(\"\\nğŸ“Š Ø®Ù„Ø§ØµÙ‡ Û±Û° Ø®ÙˆØ´Ù‡ (Ù…Ø±ØªØ¨â€ŒØ´Ø¯Ù‡ Ø¨Ø± Ø§Ø³Ø§Ø³ Ù‚ÛŒÙ…Øª):\")\n",
    "print(summary.sort_values('avg_price_mil', ascending=False))\n",
    "\n",
    "# ---------- 14. Ù†Ú©ØªÙ‡ ØªØ­Ù„ÛŒÙ„ÛŒ ----------\n",
    "print(\"\\nğŸ’¡ Ù†Ú©ØªÙ‡:\")\n",
    "print(\"- Ø¨Ø±Ø®ÛŒ Ø®ÙˆØ´Ù‡â€ŒÙ‡Ø§ Ù…Ù…Ú©Ù† Ø§Ø³Øª Ú©ÙˆÚ†Ú© Ø¨Ø§Ø´Ù†Ø¯ (Ù…Ø«Ù„ Ø®ÙˆØ´Ù‡â€ŒÙ‡Ø§ÛŒ ÙˆÛŒÚ˜Ù‡: 'ÙˆÛŒÙ„Ø§Ù‡Ø§ÛŒ Ø³Ø§Ø­Ù„ÛŒ Ø¨Ø§ Ø§Ø³ØªØ®Ø±')\")\n",
    "print(\"- Ø§ÛŒÙ† Ø·Ø¨ÛŒØ¹ÛŒ Ø§Ø³Øª â€” Ú†ÙˆÙ† K=10 Ø§Ø² Ù‚Ø¨Ù„ ØªØ¹ÛŒÛŒÙ† Ø´Ø¯Ù‡ Ùˆ Ù…Ù…Ú©Ù† Ø§Ø³Øª Ø¨Ø±Ø®ÛŒ Ø®ÙˆØ´Ù‡â€ŒÙ‡Ø§ ØªØ®ØµØµÛŒ Ø¨Ø§Ø´Ù†Ø¯.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7562c218",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------\n",
    "# ğŸ”§ Ø®ÙˆØ´Ù‡â€ŒØ¨Ù†Ø¯ÛŒ Ø§Ù…Ù„Ø§Ú© â€” Ù†Ø³Ø®Ù‡ Ù†Ù‡Ø§ÛŒÛŒ (Ø¨Ø¯ÙˆÙ† outlier)\n",
    "# --------------------------------------------\n",
    "\n",
    "df = data.copy()\n",
    "\n",
    "\n",
    "df['rooms_num'] = df['rooms_count'].apply(parse_rooms).fillna(2)\n",
    "\n",
    "# ---------- 4. Ù…Ø­Ø§Ø³Ø¨Ù‡ transformable_price ----------\n",
    "# df['rent_val'] = pd.to_numeric(df['rent_value'], errors='coerce')\n",
    "# df['cred_val'] = pd.to_numeric(df['credit_value'], errors='coerce')\n",
    "df['price_val'] = pd.to_numeric(df['price_value'], errors='coerce')\n",
    "\n",
    "# def calc_transformable(row):\n",
    "#     if 'sell' in str(row['cat2_slug']).lower() and row['price_val'] > 0:\n",
    "#         return row['price_val']\n",
    "#     if row['cred_val'] > 0 or row['rent_val'] > 0:\n",
    "#         return (row['cred_val'] or 0) + (row['rent_val'] or 0) * 1200\n",
    "#     return np.nan\n",
    "\n",
    "# df['tp'] = df.apply(calc_transformable, axis=1)\n",
    "df['tp']=df['price_val'] \n",
    "# ---------- 5. ÙÛŒÙ„ØªØ± Ø§ÙˆÙ„ÛŒÙ‡: ÙÙ‚Ø· Ø±Ú©ÙˆØ±Ø¯Ù‡Ø§ÛŒ Ù…Ø¹ØªØ¨Ø± ----------\n",
    "df = df[\n",
    "    (df['tp'].notna()) & (df['tp'] > 0) &\n",
    "    (df['building_size'].notna()) & (df['building_size'] > 5) &\n",
    "    (df['location_latitude'].notna()) & (df['location_longitude'].notna())\n",
    "].copy()\n",
    "\n",
    "print(f\"âœ… Ù¾Ø³ Ø§Ø² ÙÛŒÙ„ØªØ± Ø§ÙˆÙ„ÛŒÙ‡: {len(df)} Ø±Ú©ÙˆØ±Ø¯\")\n",
    "\n",
    "# ---------- 6. ØªØ¨Ø¯ÛŒÙ„ Ù…Ø®ØªØµØ§Øª Ø¨Ù‡ UTM (Zone 39N) ----------\n",
    "transformer = Transformer.from_crs(\"EPSG:4326\", \"EPSG:32639\")\n",
    "valid_mask = (\n",
    "    df['location_latitude'].between(25, 40) &\n",
    "    df['location_longitude'].between(44, 63)\n",
    ")\n",
    "df = df[valid_mask]\n",
    "\n",
    "lat_utm, lon_utm = transformer.transform(\n",
    "    df['location_latitude'].values,\n",
    "    df['location_longitude'].values\n",
    ")\n",
    "df['lat_utm'] = lat_utm\n",
    "df['lon_utm'] = lon_utm\n",
    "\n",
    "# ---------- 7. Ø­Ø°Ù outlierÙ‡Ø§ÛŒ Ø´Ø¯ÛŒØ¯ (Ø­ÛŒØ§ØªÛŒ!) ----------\n",
    "print(f\"Ù‚Ø¨Ù„ Ø§Ø² ÙÛŒÙ„ØªØ± outlier: max size = {df['building_size'].max():.0f}, max price = {df['tp'].max():.0f}\")\n",
    "\n",
    "# ğŸš« Ø­Ø°Ù:\n",
    "# - Ø²ÛŒØ±Ø¨Ù†Ø§ > 50,000 Ù…ØªØ± (ØºÛŒØ±ÙˆØ§Ù‚Ø¹ÛŒ Ø¨Ø±Ø§ÛŒ Ù…Ø³Ú©ÙˆÙ†ÛŒ)\n",
    "# - Ù‚ÛŒÙ…Øª > 200 Ù…ÛŒÙ„ÛŒØ§Ø±Ø¯ ØªÙˆÙ…Ø§Ù† (Ù¾Ø±Øª Ø´Ø¯ÛŒØ¯)\n",
    "# - Ø²ÛŒØ±Ø¨Ù†Ø§ < 10 Ù…ØªØ± (Ø®Ø±Ø§Ø¨)\n",
    "df_clean = df[\n",
    "    (df['building_size'] > 10) &\n",
    "    (df['building_size'] <= 50_000) &\n",
    "    (df['tp'] <= 200_000_000_000)  # 200 Ù…ÛŒÙ„ÛŒØ§Ø±Ø¯\n",
    "]\n",
    "\n",
    "print(f\"âœ… Ù¾Ø³ Ø§Ø² Ø­Ø°Ù outlier: {len(df_clean)} Ø±Ú©ÙˆØ±Ø¯\")\n",
    "print(f\"   â†’ max size = {df_clean['building_size'].max():.0f}, max price = {df_clean['tp'].max():.0f}\")\n",
    "\n",
    "# ---------- 8. Ø³Ø§Ø®Øª ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ù†Ù‡Ø§ÛŒÛŒ ----------\n",
    "# Ø§Ù…ØªÛŒØ§Ø² Ø§Ù…Ú©Ø§Ù†Ø§Øª (0â€“3)\n",
    "for col in ['has_parking', 'has_elevator', 'has_security_guard']:\n",
    "    df_clean[col] = df_clean[col].astype(str).str.lower().isin(['true', '1', 'yes'])\n",
    "df_clean['amenities'] = (\n",
    "    df_clean['has_parking'].astype(int) +\n",
    "    df_clean['has_elevator'].astype(int) +\n",
    "    df_clean['has_security_guard'].astype(int)\n",
    ")\n",
    "\n",
    "features = [\n",
    "    'tp',               # Ù‚ÛŒÙ…Øª ØªØ¨Ø¯ÛŒÙ„â€ŒØ´Ø¯Ù‡\n",
    "    'building_size',   # Ø²ÛŒØ±Ø¨Ù†Ø§\n",
    "    'rooms_num',       # ØªØ¹Ø¯Ø§Ø¯ Ø§ØªØ§Ù‚\n",
    "    'lat_utm',         # Ù…Ø®ØªØµØ§Øª\n",
    "    'lon_utm',\n",
    "    'amenities'        # Ø§Ù…ØªÛŒØ§Ø² Ø§Ù…Ú©Ø§Ù†Ø§Øª\n",
    "]\n",
    "\n",
    "X = df_clean[features].copy()\n",
    "\n",
    "# ---------- 9. StandardScaler ----------\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# ---------- 10. PCA (90% ÙˆØ§Ø±ÛŒØ§Ù†Ø³) ----------\n",
    "pca = PCA()\n",
    "pca.fit(X_scaled)\n",
    "cumsum = np.cumsum(pca.explained_variance_ratio_)\n",
    "n_comp = np.argmax(cumsum >= 0.90) + 1\n",
    "\n",
    "pca = PCA(n_components=n_comp, random_state=42)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "print(f\"\\nğŸ“Š PCA: {n_comp} Ù…Ø¤Ù„ÙÙ‡ Ø¨Ø±Ø§ÛŒ 90% ÙˆØ§Ø±ÛŒØ§Ù†Ø³ (ÙˆØ§Ø±ÛŒØ§Ù†Ø³: {cumsum[n_comp-1]:.1%})\")\n",
    "print(\"ÙˆØ²Ù†â€ŒÙ‡Ø§ÛŒ Ù…Ø¤Ù„ÙÙ‡ Ø§ÙˆÙ„ (PC1):\")\n",
    "pc1_weights = pd.Series(pca.components_[0], index=features).round(3)\n",
    "print(pc1_weights)\n",
    "\n",
    "# ---------- 11. KMeans (K=10 â€” Ù…Ù†Ø·Ù‚ÛŒ Ø¨Ø±Ø§ÛŒ 2000 Ø±Ú©ÙˆØ±Ø¯) ----------\n",
    "kmeans = KMeans(n_clusters=10, random_state=42, n_init=20)\n",
    "clusters = kmeans.fit_predict(X_pca)\n",
    "df_clean['cluster'] = clusters\n",
    "\n",
    "# ---------- 12. Ø±Ø³Ù… Ù†Ù…ÙˆØ¯Ø§Ø± ----------\n",
    "df_plot = df_clean.copy()\n",
    "df_plot['PC1'] = X_pca[:, 0]\n",
    "df_plot['PC2'] = X_pca[:, 1] if X_pca.shape[1] > 1 else 0\n",
    "df_plot['price_mil'] = df_plot['tp'] / 1_000_000\n",
    "\n",
    "fig = px.scatter(\n",
    "    df_plot,\n",
    "    x='PC1',\n",
    "    y='PC2',\n",
    "    color='cluster',\n",
    "    color_continuous_scale='Rainbow',\n",
    "    labels={'PC1': 'Ù…Ø¤Ù„ÙÙ‡ Ø§ØµÙ„ÛŒ Û±', 'PC2': 'Ù…Ø¤Ù„ÙÙ‡ Ø§ØµÙ„ÛŒ Û²'},\n",
    "    title=f'Ø®ÙˆØ´Ù‡â€ŒØ¨Ù†Ø¯ÛŒ KMeans (K=10) Ø±ÙˆÛŒ ÙØ¶Ø§ÛŒ PCA â€” {len(df_plot)} Ø±Ú©ÙˆØ±Ø¯ Ù¾Ø³ Ø§Ø² Ø­Ø°Ù outlier',\n",
    "    opacity=0.7,\n",
    "    width=950, height=600\n",
    ")\n",
    "\n",
    "# Ù…Ø±Ø§Ú©Ø² Ø®ÙˆØ´Ù‡â€ŒÙ‡Ø§\n",
    "centers_pca = kmeans.cluster_centers_\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=centers_pca[:, 0], \n",
    "    y=centers_pca[:, 1] if centers_pca.shape[1] > 1 else [0]*len(centers_pca),\n",
    "    mode='markers+text',\n",
    "    marker=dict(size=18, color='black', symbol='star', line=dict(width=2, color='white')),\n",
    "    text=[f'C{i}' for i in range(5)],\n",
    "    textposition=\"top center\",\n",
    "    name='Ù…Ø±Ø§Ú©Ø² Ø®ÙˆØ´Ù‡â€ŒÙ‡Ø§'\n",
    "))\n",
    "\n",
    "fig.update_layout(font=dict(size=13), title_font_size=16)\n",
    "fig.write_html(\"kmeans_final.html\")\n",
    "fig.show()\n",
    "\n",
    "# ---------- 13. Ø®Ù„Ø§ØµÙ‡ Ø®ÙˆØ´Ù‡â€ŒÙ‡Ø§ ----------\n",
    "summary = df_plot.groupby('cluster').agg(\n",
    "    count=('tp', 'size'),\n",
    "    avg_price_mil=('price_mil', 'mean'),\n",
    "    avg_size=('building_size', 'mean'),\n",
    "    avg_rooms=('rooms_num', 'mean'),\n",
    "    avg_amenities=('amenities', 'mean'),\n",
    "    main_city=('city_slug', lambda x: x.value_counts().index[0] if len(x.value_counts()) > 0 else 'â€”')\n",
    ").round(2)\n",
    "\n",
    "print(\"\\nğŸ“Š Ø®Ù„Ø§ØµÙ‡ Ù†Ù‡Ø§ÛŒÛŒ Ø®ÙˆØ´Ù‡â€ŒÙ‡Ø§:\")\n",
    "print(summary.sort_values('avg_price_mil', ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6337c3a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------\n",
    "# âœ… KMeans Ø¨Ø§ K=10 â€” ÙÙ‚Ø· Ø¢Ú¯Ù‡ÛŒâ€ŒÙ‡Ø§ÛŒ ÙØ±ÙˆØ´ÛŒ + ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ù‡ÙˆØ´Ù…Ù†Ø¯Ø§Ù†Ù‡\n",
    "#    âœ… ÙÙ‚Ø· price_value (Ù†Ù‡ transformable_price)\n",
    "#    âœ… Ø³Ø§Ù„ Ø³Ø§Ø®Øª Ùˆ Ø´Ù‡Ø± Ø¨Ù‡ Ø¹Ù†ÙˆØ§Ù† ÙÛŒÚ†Ø±Ù‡Ø§ÛŒ Ú©Ù„ÛŒØ¯ÛŒ\n",
    "# --------------------------------------------------\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from pyproj import Transformer\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.cluster import KMeans\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "\n",
    "df = data.copy()\n",
    "\n",
    "# ---------- 2. ØªØ¨Ø¯ÛŒÙ„ Ø§Ø¹Ø¯Ø§Ø¯ ÙØ§Ø±Ø³ÛŒ ----------\n",
    "def persian_to_english_digits(text):\n",
    "    if not isinstance(text, str): return text\n",
    "    return text.translate(str.maketrans(\"Û°Û±Û²Û³Û´ÛµÛ¶Û·Û¸Û¹Ù Ù¡Ù¢Ù£Ù¤Ù¥Ù¦Ù§Ù¨Ù©\", \"01234567890123456789\"))\n",
    "\n",
    "\n",
    "# ---------- 3. ØªØ¨Ø¯ÛŒÙ„ rooms_count ----------\n",
    "def parse_rooms(val):\n",
    "    if pd.isna(val): return np.nan\n",
    "    s = str(val).strip().lower()\n",
    "    if 'Ø¨Ø¯ÙˆÙ†' in s: return 0\n",
    "    if 'ÛŒÚ©' in s: return 1\n",
    "    if 'Ø¯Ùˆ' in s: return 2\n",
    "    if 'Ø³Ù‡' in s: return 3\n",
    "    if 'Ú†Ù‡Ø§Ø±' in s: return 4\n",
    "    if 'Ù¾Ù†Ø¬' in s: return 5\n",
    "    nums = re.findall(r'\\d+', s)\n",
    "    return int(nums[0]) if nums else np.nan\n",
    "\n",
    "df['rooms_num'] = df['rooms_count'].apply(parse_rooms).fillna(2)\n",
    "\n",
    "# ---------- 4. Ù¾Ø§Ú©â€ŒØ³Ø§Ø²ÛŒ Ø³Ø§Ù„ Ø³Ø§Ø®Øª ----------\n",
    "def parse_year(val):\n",
    "    if pd.isna(val): return np.nan\n",
    "    s = str(val).strip()\n",
    "    s = persian_to_english_digits(s)\n",
    "    nums = re.findall(r'1[3-4]\\d{2}', s)  # Ø³Ø§Ù„ 1300 ØªØ§ 1499\n",
    "    return int(nums[0]) if nums else np.nan\n",
    "\n",
    "df['year'] = df['construction_year'].apply(parse_year)\n",
    "\n",
    "# ---------- 5. ÙÛŒÙ„ØªØ± ÙÙ‚Ø· Ø¢Ú¯Ù‡ÛŒâ€ŒÙ‡Ø§ÛŒ ÙØ±ÙˆØ´ÛŒ ----------\n",
    "df_sell = df[\n",
    "    (df['cat2_slug'].str.contains('sell', case=False, na=False)) &\n",
    "    (df['price_value'].notna()) &\n",
    "    (df['building_size'].notna()) &\n",
    "    (df['location_latitude'].between(25, 40)) &\n",
    "    (df['location_longitude'].between(44, 63))\n",
    "].copy()\n",
    "\n",
    "# ---------- 6. ØªØ¨Ø¯ÛŒÙ„ Ø¹Ø¯Ø¯ÛŒ + ÙÛŒÙ„ØªØ± Ù…Ù†Ø·Ù‚ÛŒ ----------\n",
    "df_sell['price'] = pd.to_numeric(df_sell['price_value'], errors='coerce')\n",
    "df_sell['size'] = pd.to_numeric(df_sell['building_size'], errors='coerce')\n",
    "\n",
    "df_sell = df_sell[\n",
    "    (df_sell['price'] > 10_000_000) & (df_sell['price'] <= 200_000_000_000) &\n",
    "    (df_sell['size'] > 10) & (df_sell['size'] <= 50_000) &\n",
    "    (df_sell['year'].between(1300, 1405))  # ÙÙ‚Ø· Ø³Ø§Ù„â€ŒÙ‡Ø§ÛŒ Ù…Ø¹Ù‚ÙˆÙ„\n",
    "]\n",
    "\n",
    "print(f\"âœ… Ø¢Ú¯Ù‡ÛŒâ€ŒÙ‡Ø§ÛŒ ÙØ±ÙˆØ´ÛŒ Ù…Ø¹ØªØ¨Ø±: {len(df_sell)}\")\n",
    "\n",
    "# ---------- 7. ØªØ¨Ø¯ÛŒÙ„ Ù…Ø®ØªØµØ§Øª Ø¨Ù‡ UTM ----------\n",
    "transformer = Transformer.from_crs(\"EPSG:4326\", \"EPSG:32639\")\n",
    "lat_utm, lon_utm = transformer.transform(\n",
    "    df_sell['location_latitude'].values,\n",
    "    df_sell['location_longitude'].values\n",
    ")\n",
    "df_sell['lat_utm'] = lat_utm\n",
    "df_sell['lon_utm'] = lon_utm\n",
    "\n",
    "# ---------- 8. Ú©Ø¯Ú¯Ø°Ø§Ø±ÛŒ Ø´Ù‡Ø± (LabelEncoder) ----------\n",
    "le_city = LabelEncoder()\n",
    "# ÙÙ‚Ø· Ø´Ù‡Ø±Ù‡Ø§ÛŒ Ø¨Ø§ â‰¥10 Ø¢Ú¯Ù‡ÛŒ\n",
    "city_counts = df_sell['city_slug'].value_counts()\n",
    "valid_cities = city_counts[city_counts >= 10].index\n",
    "df_sell['city_encoded'] = df_sell['city_slug'].where(df_sell['city_slug'].isin(valid_cities), 'Ø³Ø§ÛŒØ±')\n",
    "df_sell['city_encoded'] = le_city.fit_transform(df_sell['city_encoded'])\n",
    "\n",
    "# ---------- 9. ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ù†Ù‡Ø§ÛŒÛŒ (5 Ù…ÙˆØ±Ø¯ â€” Ú©Ù…ÛŒÙ†Ù‡ Ùˆ Ù‡ÙˆØ´Ù…Ù†Ø¯Ø§Ù†Ù‡) ----------\n",
    "features = [\n",
    "    'price',          # Ù‚ÛŒÙ…Øª ÙØ±ÙˆØ´ (ØªÙˆÙ…Ø§Ù†)\n",
    "    'size',           # Ø²ÛŒØ±Ø¨Ù†Ø§ (Ù…ØªØ±)\n",
    "    'rooms_num',      # ØªØ¹Ø¯Ø§Ø¯ Ø§ØªØ§Ù‚\n",
    "    'year',           # Ø³Ø§Ù„ Ø³Ø§Ø®Øª â€” âœ… Ø¬Ø§ÛŒÚ¯Ø²ÛŒÙ† amenity\n",
    "    'city_encoded'    # Ø´Ù‡Ø± â€” âœ… Ø¬Ø§ÛŒÚ¯Ø²ÛŒÙ† UTM (ØªÙØ³ÛŒØ±Ù¾Ø°ÛŒØ±ØªØ±)\n",
    "]\n",
    "\n",
    "X = df_sell[features].copy().dropna()\n",
    "\n",
    "print(f\"âœ… Ù¾Ø³ Ø§Ø² Ø­Ø°Ù NaN: {len(X)} Ø±Ú©ÙˆØ±Ø¯\")\n",
    "\n",
    "# ---------- 10. StandardScaler ----------\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# ---------- 11. âœ… KMeans Ø¨Ø§ K=10 ----------\n",
    "kmeans = KMeans(n_clusters=10, random_state=42, n_init=25)\n",
    "clusters = kmeans.fit_predict(X_scaled)\n",
    "df_sell = df_sell.loc[X.index].copy()  # Ù‡Ù…Ø§Ù‡Ù†Ú¯ÛŒ Ø§ÛŒÙ†Ø¯Ú©Ø³\n",
    "df_sell['cluster'] = clusters\n",
    "\n",
    "# ---------- 12. Ø±Ø³Ù…: Ù‚ÛŒÙ…Øª vs Ø²ÛŒØ±Ø¨Ù†Ø§ ----------\n",
    "df_sell['price_mil'] = df_sell['price'] / 1_000_000\n",
    "\n",
    "fig = px.scatter(\n",
    "    df_sell,\n",
    "    x=lat_utm,\n",
    "    y=lon_utm,\n",
    "    color='cluster',\n",
    "    color_continuous_scale='Rainbow',\n",
    "    labels={\n",
    "        'size': 'Ø²ÛŒØ±Ø¨Ù†Ø§ (Ù…ØªØ± Ù…Ø±Ø¨Ø¹)',\n",
    "        'price_mil': 'Ù‚ÛŒÙ…Øª ÙØ±ÙˆØ´ (Ù…ÛŒÙ„ÛŒÙˆÙ† ØªÙˆÙ…Ø§Ù†)',\n",
    "        'cluster': 'Ø®ÙˆØ´Ù‡'\n",
    "    },\n",
    "    title='Ø®ÙˆØ´Ù‡â€ŒØ¨Ù†Ø¯ÛŒ KMeans â€” ÙÙ‚Ø· ÙØ±ÙˆØ´ÛŒâ€ŒÙ‡Ø§ (5 ÙˆÛŒÚ˜Ú¯ÛŒ Ù…Ø¹Ù†Ø§Ø¯Ø§Ø±)',\n",
    "    opacity=0.65,\n",
    "    width=950, height=650\n",
    ")\n",
    "\n",
    "# Ù…Ø±Ø§Ú©Ø² Ø®ÙˆØ´Ù‡â€ŒÙ‡Ø§\n",
    "centers_scaled = kmeans.cluster_centers_\n",
    "centers_original = scaler.inverse_transform(centers_scaled)\n",
    "centers_df = pd.DataFrame(centers_original, columns=features)\n",
    "\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=centers_df['size'],\n",
    "    y=centers_df['price'] / 1_000_000,\n",
    "    mode='markers+text',\n",
    "    marker=dict(size=18, color='black', symbol='star', line=dict(width=2, color='white')),\n",
    "    text=[f'C{i}' for i in range(10)],\n",
    "    textposition=\"top center\",\n",
    "    name='Ù…Ø±Ø§Ú©Ø² Ø®ÙˆØ´Ù‡â€ŒÙ‡Ø§'\n",
    "))\n",
    "\n",
    "fig.update_layout(font=dict(size=13), title_font_size=18)\n",
    "fig.write_html(\"kmeans_sell_optimized.html\")\n",
    "fig.show()\n",
    "\n",
    "# ---------- 13. Ø®Ù„Ø§ØµÙ‡ Ø®ÙˆØ´Ù‡â€ŒÙ‡Ø§ ----------\n",
    "summary = df_sell.groupby('cluster').agg(\n",
    "    count=('price', 'size'),\n",
    "    avg_price_mil=('price_mil', 'mean'),\n",
    "    avg_size=('size', 'mean'),\n",
    "    avg_rooms=('rooms_num', 'mean'),\n",
    "    avg_year=('year', 'mean'),\n",
    "    main_city=('city_slug', lambda x: x.mode().iloc[0] if not x.mode().empty else 'â€”')\n",
    ").round(2)\n",
    "\n",
    "print(\"\\nğŸ“Š Ø®Ù„Ø§ØµÙ‡ Û±Û° Ø®ÙˆØ´Ù‡ (Ø¨Ø§ Ø³Ø§Ù„ Ø³Ø§Ø®Øª Ùˆ Ø´Ù‡Ø±):\")\n",
    "print(summary.sort_values('avg_price_mil', ascending=False))\n",
    "\n",
    "# ---------- 14. ØªØ­Ù„ÛŒÙ„ Ø®ÙˆØ´Ù‡â€ŒÙ‡Ø§ÛŒ Ú©Ù„ÛŒØ¯ÛŒ ----------\n",
    "high_year = summary[summary['avg_year'] > 1395]\n",
    "old_year = summary[summary['avg_year'] < 1370]\n",
    "\n",
    "print(\"\\nğŸ” ÛŒØ§ÙØªÙ‡â€ŒÙ‡Ø§ÛŒ Ø¬Ø§Ù„Ø¨:\")\n",
    "if not high_year.empty:\n",
    "    print(f\"â€¢ Ø®Ø§Ù†Ù‡â€ŒÙ‡Ø§ÛŒ Ø¬Ø¯ÛŒØ¯Ø³Ø§Ø®Øª ({high_year.index.tolist()}): Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† Ø³Ø§Ù„ = {high_year['avg_year'].mean():.0f}\")\n",
    "if not old_year.empty:\n",
    "    print(f\"â€¢ Ø®Ø§Ù†Ù‡â€ŒÙ‡Ø§ÛŒ Ù‚Ø¯ÛŒÙ…ÛŒ ({old_year.index.tolist()}): Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† Ø³Ø§Ù„ = {old_year['avg_year'].mean():.0f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ef7409",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------\n",
    "# âœ… KMeans + PCA â€” ÙÙ‚Ø· Ø¢Ú¯Ù‡ÛŒâ€ŒÙ‡Ø§ÛŒ ÙØ±ÙˆØ´ÛŒ + Ø³Ø§Ù„ Ø³Ø§Ø®Øª + Ø´Ù‡Ø±\n",
    "#    âœ… Ø¨Ø¯ÙˆÙ† transformable_price\n",
    "#    âœ… Ø¨Ø§ PCA Ø¨Ø±Ø§ÛŒ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø³ÛŒÚ¯Ù†Ø§Ù„ Ø§ØµÙ„ÛŒ\n",
    "# --------------------------------------------------\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "\n",
    "df = data.copy()\n",
    "\n",
    "# ---------- 2. ØªØ¨Ø¯ÛŒÙ„ Ø§Ø¹Ø¯Ø§Ø¯ ÙØ§Ø±Ø³ÛŒ ----------\n",
    "def persian_to_english_digits(text):\n",
    "    if not isinstance(text, str): return text\n",
    "    return text.translate(str.maketrans(\"Û°Û±Û²Û³Û´ÛµÛ¶Û·Û¸Û¹Ù Ù¡Ù¢Ù£Ù¤Ù¥Ù¦Ù§Ù¨Ù©\", \"01234567890123456789\"))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df['rooms_num'] = df['rooms_count'].apply(parse_rooms).fillna(2)\n",
    "\n",
    "# ---------- 4. Ù¾Ø§Ú©â€ŒØ³Ø§Ø²ÛŒ Ø³Ø§Ù„ Ø³Ø§Ø®Øª ----------\n",
    "def parse_year(val):\n",
    "    if pd.isna(val): return np.nan\n",
    "    s = str(val).strip()\n",
    "    s = persian_to_english_digits(s)\n",
    "    nums = re.findall(r'1[3-4]\\d{2}', s)\n",
    "    return int(nums[0]) if nums else np.nan\n",
    "\n",
    "df['year'] = df['construction_year'].apply(parse_year)\n",
    "\n",
    "# ---------- 5. ÙÛŒÙ„ØªØ± ÙÙ‚Ø· Ø¢Ú¯Ù‡ÛŒâ€ŒÙ‡Ø§ÛŒ ÙØ±ÙˆØ´ÛŒ ----------\n",
    "df_sell = df[\n",
    "    (df['cat2_slug'].str.contains('sell', case=False, na=False)) &\n",
    "    (df['price_value'].notna()) &\n",
    "    (df['building_size'].notna()) &\n",
    "    (df['location_latitude'].between(25, 40)) &\n",
    "    (df['location_longitude'].between(44, 63))\n",
    "].copy()\n",
    "\n",
    "# ---------- 6. ØªØ¨Ø¯ÛŒÙ„ Ø¹Ø¯Ø¯ÛŒ + ÙÛŒÙ„ØªØ± Ù…Ù†Ø·Ù‚ÛŒ ----------\n",
    "df_sell['price'] = pd.to_numeric(df_sell['price_value'], errors='coerce')\n",
    "df_sell['size'] = pd.to_numeric(df_sell['building_size'], errors='coerce')\n",
    "\n",
    "df_sell = df_sell[\n",
    "    (df_sell['price'] > 10_000_000) & (df_sell['price'] <= 200_000_000_000) &\n",
    "    (df_sell['size'] > 10) & (df_sell['size'] <= 50_000) &\n",
    "    (df_sell['year'].between(1300, 1405))\n",
    "]\n",
    "\n",
    "print(f\"âœ… Ø¢Ú¯Ù‡ÛŒâ€ŒÙ‡Ø§ÛŒ ÙØ±ÙˆØ´ÛŒ Ù…Ø¹ØªØ¨Ø±: {len(df_sell)}\")\n",
    "\n",
    "# ---------- 7. Ú©Ø¯Ú¯Ø°Ø§Ø±ÛŒ Ø´Ù‡Ø± ----------\n",
    "le_city = LabelEncoder()\n",
    "city_counts = df_sell['city_slug'].value_counts()\n",
    "valid_cities = city_counts[city_counts >= 10].index\n",
    "df_sell['city_encoded'] = df_sell['city_slug'].where(df_sell['city_slug'].isin(valid_cities), 'Ø³Ø§ÛŒØ±')\n",
    "df_sell['city_encoded'] = le_city.fit_transform(df_sell['city_encoded'])\n",
    "\n",
    "# ---------- 8. ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ù†Ù‡Ø§ÛŒÛŒ (5 Ù…ÙˆØ±Ø¯) ----------\n",
    "features = [\n",
    "    'price',          # Ù‚ÛŒÙ…Øª ÙØ±ÙˆØ´\n",
    "    'size',           # Ø²ÛŒØ±Ø¨Ù†Ø§\n",
    "    'rooms_num',      # ØªØ¹Ø¯Ø§Ø¯ Ø§ØªØ§Ù‚\n",
    "    'year',           # Ø³Ø§Ù„ Ø³Ø§Ø®Øª âœ…\n",
    "    'city_encoded'    # Ø´Ù‡Ø± âœ…\n",
    "]\n",
    "\n",
    "X = df_sell[features].copy().dropna()\n",
    "print(f\"âœ… Ù¾Ø³ Ø§Ø² Ø­Ø°Ù NaN: {len(X)} Ø±Ú©ÙˆØ±Ø¯\")\n",
    "\n",
    "# ---------- 9. StandardScaler ----------\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# ---------- 10. PCA (90% ÙˆØ§Ø±ÛŒØ§Ù†Ø³) ----------\n",
    "pca = PCA()\n",
    "pca.fit(X_scaled)\n",
    "cumsum = np.cumsum(pca.explained_variance_ratio_)\n",
    "n_comp = np.argmax(cumsum >= 0.90) + 1\n",
    "\n",
    "pca = PCA(n_components=n_comp, random_state=42)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "print(f\"\\nğŸ“Š PCA: {n_comp} Ù…Ø¤Ù„ÙÙ‡ Ø¨Ø±Ø§ÛŒ 90% ÙˆØ§Ø±ÛŒØ§Ù†Ø³\")\n",
    "print(\"ÙˆØ²Ù†â€ŒÙ‡Ø§ÛŒ Ù…Ø¤Ù„ÙÙ‡ Ø§ÙˆÙ„ (PC1):\")\n",
    "pc1_weights = pd.Series(pca.components_[0], index=features).round(3)\n",
    "print(pc1_weights)\n",
    "\n",
    "# ---------- 11. âœ… KMeans Ø¨Ø§ K=10 Ø±ÙˆÛŒ ÙØ¶Ø§ÛŒ PCA ----------\n",
    "kmeans = KMeans(n_clusters=10, random_state=42, n_init=25)\n",
    "clusters = kmeans.fit_predict(X_pca)\n",
    "df_sell = df_sell.loc[X.index].copy()\n",
    "df_sell['cluster'] = clusters\n",
    "\n",
    "# ---------- 12. Ø±Ø³Ù… Ø¯Ø± ÙØ¶Ø§ÛŒ PCA ----------\n",
    "df_plot = df_sell.copy()\n",
    "df_plot['PC1'] = X_pca[:, 0]\n",
    "df_plot['PC2'] = X_pca[:, 1] if X_pca.shape[1] > 1 else 0\n",
    "df_plot['price_mil'] = df_plot['price'] / 1_000_000\n",
    "\n",
    "fig = px.scatter(\n",
    "    df_plot,\n",
    "    x='PC1',\n",
    "    y='PC2',\n",
    "    color='cluster',\n",
    "    color_continuous_scale='Rainbow',\n",
    "    labels={'PC1': 'Ù…Ø¤Ù„ÙÙ‡ Ø§ØµÙ„ÛŒ Û±', 'PC2': 'Ù…Ø¤Ù„ÙÙ‡ Ø§ØµÙ„ÛŒ Û²'},\n",
    "    title=f'Ø®ÙˆØ´Ù‡â€ŒØ¨Ù†Ø¯ÛŒ KMeans (K=10) Ø±ÙˆÛŒ ÙØ¶Ø§ÛŒ PCA â€” {n_comp} Ù…Ø¤Ù„ÙÙ‡ (90% ÙˆØ§Ø±ÛŒØ§Ù†Ø³)',\n",
    "    opacity=0.65,\n",
    "    width=950, height=650\n",
    ")\n",
    "\n",
    "# Ù…Ø±Ø§Ú©Ø² Ø®ÙˆØ´Ù‡â€ŒÙ‡Ø§\n",
    "centers_pca = kmeans.cluster_centers_\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=centers_pca[:, 0], \n",
    "    y=centers_pca[:, 1] if centers_pca.shape[1] > 1 else [0]*len(centers_pca),\n",
    "    mode='markers+text',\n",
    "    marker=dict(size=18, color='black', symbol='star', line=dict(width=2, color='white')),\n",
    "    text=[f'C{i}' for i in range(10)],\n",
    "    textposition=\"top center\",\n",
    "    name='Ù…Ø±Ø§Ú©Ø² Ø®ÙˆØ´Ù‡â€ŒÙ‡Ø§'\n",
    "))\n",
    "\n",
    "fig.update_layout(font=dict(size=13), title_font_size=18)\n",
    "fig.write_html(\"kmeans_pca_optimized.html\")\n",
    "fig.show()\n",
    "\n",
    "# ---------- 13. Ø®Ù„Ø§ØµÙ‡ Ø®ÙˆØ´Ù‡â€ŒÙ‡Ø§ ----------\n",
    "summary = df_plot.groupby('cluster').agg(\n",
    "    count=('price', 'size'),\n",
    "    avg_price_mil=('price_mil', 'mean'),\n",
    "    avg_size=('size', 'mean'),\n",
    "    avg_rooms=('rooms_num', 'mean'),\n",
    "    avg_year=('year', 'mean'),\n",
    "    main_city=('city_slug', lambda x: x.mode().iloc[0] if not x.mode().empty else 'â€”')\n",
    ").round(2)\n",
    "\n",
    "print(\"\\nğŸ“Š Ø®Ù„Ø§ØµÙ‡ Û±Û° Ø®ÙˆØ´Ù‡ (Ø¨Ø§ PCA + Ø³Ø§Ù„ Ø³Ø§Ø®Øª + Ø´Ù‡Ø±):\")\n",
    "print(summary.sort_values('avg_price_mil', ascending=False))\n",
    "\n",
    "# ---------- 14. ØªÙØ³ÛŒØ± Ù…Ø¤Ù„ÙÙ‡â€ŒÙ‡Ø§ ----------\n",
    "print(\"\\nğŸ” ØªÙØ³ÛŒØ± Ù…Ø¤Ù„ÙÙ‡â€ŒÙ‡Ø§ÛŒ PCA:\")\n",
    "if n_comp >= 2:\n",
    "    pc2_weights = pd.Series(pca.components_[1], index=features).round(3)\n",
    "    print(\"PC1 (Ø§ÙˆÙ„ÛŒÙ† Ù…Ø¤Ù„ÙÙ‡):\")\n",
    "    print(pc1_weights.to_string())\n",
    "    print(\"\\nPC2 (Ø¯ÙˆÙ…ÛŒÙ† Ù…Ø¤Ù„ÙÙ‡):\")\n",
    "    print(pc2_weights.to_string())\n",
    "    \n",
    "    # ØªØ­Ù„ÛŒÙ„ Ø®ÙˆØ´Ù‡â€ŒÙ‡Ø§ÛŒ Ù‚Ø¯ÛŒÙ…ÛŒ vs Ø¬Ø¯ÛŒØ¯\n",
    "    old_clusters = summary[summary['avg_year'] < 1370].index\n",
    "    new_clusters = summary[summary['avg_year'] > 1395].index\n",
    "    if len(old_clusters) > 0 and len(new_clusters) > 0:\n",
    "        print(f\"\\nâœ… ÛŒØ§ÙØªÙ‡: Ø®ÙˆØ´Ù‡â€ŒÙ‡Ø§ÛŒ Ù‚Ø¯ÛŒÙ…ÛŒ ({list(old_clusters)}) Ùˆ Ø¬Ø¯ÛŒØ¯ ({list(new_clusters)}) Ø¯Ø± ÙØ¶Ø§ÛŒ PCA Ø¨Ù‡â€ŒÙˆØ¶ÙˆØ­ ØªÙÚ©ÛŒÚ© Ø´Ø¯Ù‡â€ŒØ§Ù†Ø¯.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04fcae42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------\n",
    "# âœ… KMeans + PCA â€” Ø¨Ø§ ØªÙ…Ø§Ù… ÙÛŒÚ†Ø±Ù‡Ø§ÛŒ Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯ÛŒ Ø´Ù…Ø§\n",
    "#    âœ… ÙÙ‚Ø· ÙØ±ÙˆØ´ÛŒâ€ŒÙ‡Ø§\n",
    "#    âœ… 11 ÙˆÛŒÚ˜Ú¯ÛŒ Ù…Ø¹Ù†Ø§Ø¯Ø§Ø± (Ù…Ø§Ù„ÛŒ + Ù…Ú©Ø§Ù†ÛŒ + Ø§Ù…Ú©Ø§Ù†Ø§Øª)\n",
    "#    âœ… K=10 Ø¯Ù‚ÛŒÙ‚Ø§Ù‹ Ù…Ø·Ø§Ø¨Ù‚ ØµÙˆØ±Øª Ø³ÙˆØ§Ù„\n",
    "# --------------------------------------------------\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "\n",
    "df = data.copy()\n",
    "\n",
    "# ---------- 2. ØªØ¨Ø¯ÛŒÙ„ Ø§Ø¹Ø¯Ø§Ø¯ ÙØ§Ø±Ø³ÛŒ ----------\n",
    "def persian_to_english_digits(text):\n",
    "    if not isinstance(text, str): return text\n",
    "    return text.translate(str.maketrans(\"Û°Û±Û²Û³Û´ÛµÛ¶Û·Û¸Û¹Ù Ù¡Ù¢Ù£Ù¤Ù¥Ù¦Ù§Ù¨Ù©\", \"01234567890123456789\"))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df['rooms_num'] = df['rooms_count'].apply(parse_rooms).fillna(2)\n",
    "\n",
    "# ---------- 4. Ù¾Ø§Ú©â€ŒØ³Ø§Ø²ÛŒ Ø³Ø§Ù„ Ø³Ø§Ø®Øª ----------\n",
    "def parse_year(val):\n",
    "    if pd.isna(val): return np.nan\n",
    "    s = str(val).strip()\n",
    "    s = persian_to_english_digits(s)\n",
    "    nums = re.findall(r'1[3-4]\\d{2}', s)\n",
    "    return int(nums[0]) if nums else np.nan\n",
    "\n",
    "df['year'] = df['construction_year'].apply(parse_year)\n",
    "\n",
    "# ---------- 5. ÙÛŒÙ„ØªØ± ÙÙ‚Ø· Ø¢Ú¯Ù‡ÛŒâ€ŒÙ‡Ø§ÛŒ ÙØ±ÙˆØ´ÛŒ ----------\n",
    "df_sell = df[\n",
    "    (df['cat2_slug'].str.contains('sell', case=False, na=False)) &\n",
    "    (df['price_value'].notna()) &\n",
    "    (df['building_size'].notna()) &\n",
    "    (df['location_latitude'].between(25, 40)) &\n",
    "    (df['location_longitude'].between(44, 63))\n",
    "].copy()\n",
    "\n",
    "# ---------- 6. ØªØ¨Ø¯ÛŒÙ„ Ø¹Ø¯Ø¯ÛŒ + ÙÛŒÙ„ØªØ± Ù…Ù†Ø·Ù‚ÛŒ ----------\n",
    "df_sell['price'] = pd.to_numeric(df_sell['price_value'], errors='coerce')\n",
    "df_sell['size'] = pd.to_numeric(df_sell['building_size'], errors='coerce')\n",
    "\n",
    "df_sell = df_sell[\n",
    "    (df_sell['price'] > 10_000_000) & (df_sell['price'] <= 200_000_000_000) &\n",
    "    (df_sell['size'] > 10) & (df_sell['size'] <= 50_000) &\n",
    "    (df_sell['year'].between(1300, 1405))\n",
    "]\n",
    "\n",
    "print(f\"âœ… Ø¢Ú¯Ù‡ÛŒâ€ŒÙ‡Ø§ÛŒ ÙØ±ÙˆØ´ÛŒ Ù…Ø¹ØªØ¨Ø±: {len(df_sell)}\")\n",
    "\n",
    "# ---------- 7. ØªØ¨Ø¯ÛŒÙ„ Ø§Ù…Ú©Ø§Ù†Ø§Øª Ø¨Ù‡ Ø¨Ø§ÛŒÙ†Ø±ÛŒ (0/1) ----------\n",
    "bool_cols = [\n",
    "    'has_pool', 'has_jacuzzi', 'has_elevator',\n",
    "    'has_warehouse', 'has_parking', 'has_security_guard'\n",
    "]\n",
    "\n",
    "for col in bool_cols:\n",
    "    df_sell[col] = df_sell[col].astype(str).str.lower().isin(['true', '1', 'yes']).astype(int)\n",
    "\n",
    "# ---------- 8. Ú©Ø¯Ú¯Ø°Ø§Ø±ÛŒ Ø´Ù‡Ø± ----------\n",
    "le_city = LabelEncoder()\n",
    "city_counts = df_sell['city_slug'].value_counts()\n",
    "valid_cities = city_counts[city_counts >= 10].index\n",
    "df_sell['city_encoded'] = df_sell['city_slug'].where(df_sell['city_slug'].isin(valid_cities), 'Ø³Ø§ÛŒØ±')\n",
    "df_sell['city_encoded'] = le_city.fit_transform(df_sell['city_encoded'])\n",
    "\n",
    "# ---------- 9. ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ù†Ù‡Ø§ÛŒÛŒ (11 Ù…ÙˆØ±Ø¯) ----------\n",
    "features = [\n",
    "    'price', 'size', 'rooms_num', 'year', 'city_encoded',\n",
    "    'has_pool', 'has_jacuzzi', 'has_elevator',\n",
    "    'has_warehouse', 'has_parking', 'has_security_guard'\n",
    "]\n",
    "\n",
    "X = df_sell[features].copy().dropna()\n",
    "print(f\"âœ… Ù¾Ø³ Ø§Ø² Ø­Ø°Ù NaN: {len(X)} Ø±Ú©ÙˆØ±Ø¯\")\n",
    "\n",
    "# ---------- 10. StandardScaler ----------\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# ---------- 11. PCA (90% ÙˆØ§Ø±ÛŒØ§Ù†Ø³) ----------\n",
    "pca = PCA()\n",
    "pca.fit(X_scaled)\n",
    "cumsum = np.cumsum(pca.explained_variance_ratio_)\n",
    "n_comp = np.argmax(cumsum >= 0.90) + 1\n",
    "\n",
    "pca = PCA(n_components=n_comp, random_state=42)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "print(f\"\\nğŸ“Š PCA: {n_comp} Ù…Ø¤Ù„ÙÙ‡ Ø¨Ø±Ø§ÛŒ 90% ÙˆØ§Ø±ÛŒØ§Ù†Ø³\")\n",
    "print(\"ØªÙˆØ¶ÛŒØ­ ÙˆØ§Ø±ÛŒØ§Ù†Ø³ Ù‡Ø± Ù…Ø¤Ù„ÙÙ‡:\", pca.explained_variance_ratio_.round(3))\n",
    "\n",
    "# ---------- 12. âœ… KMeans Ø¨Ø§ K=10 ----------\n",
    "kmeans = KMeans(n_clusters=10, random_state=42, n_init=25)\n",
    "clusters = kmeans.fit_predict(X_pca)\n",
    "df_sell = df_sell.loc[X.index].copy()\n",
    "df_sell['cluster'] = clusters\n",
    "\n",
    "# ---------- 13. Ø±Ø³Ù… Ø¯Ø± ÙØ¶Ø§ÛŒ PCA ----------\n",
    "df_plot = df_sell.copy()\n",
    "df_plot['PC1'] = X_pca[:, 0]\n",
    "df_plot['PC2'] = X_pca[:, 1] if X_pca.shape[1] > 1 else 0\n",
    "df_plot['price_mil'] = df_plot['price'] / 1_000_000\n",
    "\n",
    "fig = px.scatter(\n",
    "    df_plot,\n",
    "    x='PC1',\n",
    "    y='PC2',\n",
    "    color='cluster',\n",
    "    color_continuous_scale='Rainbow',\n",
    "    labels={'PC1': 'Ù…Ø¤Ù„ÙÙ‡ Ø§ØµÙ„ÛŒ Û±', 'PC2': 'Ù…Ø¤Ù„ÙÙ‡ Ø§ØµÙ„ÛŒ Û²'},\n",
    "    title=f'Ø®ÙˆØ´Ù‡â€ŒØ¨Ù†Ø¯ÛŒ KMeans (K=10) â€” 11 ÙˆÛŒÚ˜Ú¯ÛŒ + PCA ({n_comp} Ù…Ø¤Ù„ÙÙ‡)',\n",
    "    opacity=0.6,\n",
    "    width=950, height=650\n",
    ")\n",
    "\n",
    "# Ù…Ø±Ø§Ú©Ø² Ø®ÙˆØ´Ù‡â€ŒÙ‡Ø§\n",
    "centers_pca = kmeans.cluster_centers_\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=centers_pca[:, 0], \n",
    "    y=centers_pca[:, 1] if centers_pca.shape[1] > 1 else [0]*len(centers_pca),\n",
    "    mode='markers+text',\n",
    "    marker=dict(size=18, color='black', symbol='star', line=dict(width=2, color='white')),\n",
    "    text=[f'C{i}' for i in range(10)],\n",
    "    textposition=\"top center\",\n",
    "    name='Ù…Ø±Ø§Ú©Ø² Ø®ÙˆØ´Ù‡â€ŒÙ‡Ø§'\n",
    "))\n",
    "\n",
    "fig.update_layout(font=dict(size=13), title_font_size=18)\n",
    "fig.write_html(\"kmeans_pca_full_features.html\")\n",
    "fig.show()\n",
    "\n",
    "# ---------- 14. Ø®Ù„Ø§ØµÙ‡ Ø®ÙˆØ´Ù‡â€ŒÙ‡Ø§ ----------\n",
    "summary = df_plot.groupby('cluster').agg(\n",
    "    count=('price', 'size'),\n",
    "    avg_price_mil=('price_mil', 'mean'),\n",
    "    avg_size=('size', 'mean'),\n",
    "    avg_rooms=('rooms_num', 'mean'),\n",
    "    avg_year=('year', 'mean'),\n",
    "    main_city=('city_slug', lambda x: x.mode().iloc[0] if not x.mode().empty else 'â€”'),\n",
    "    pool=('has_pool', 'mean'),\n",
    "    jacuzzi=('has_jacuzzi', 'mean'),\n",
    "    elevator=('has_elevator', 'mean'),\n",
    "    parking=('has_parking', 'mean'),\n",
    "    guard=('has_security_guard', 'mean')\n",
    ").round(3)\n",
    "\n",
    "print(\"\\nğŸ“Š Ø®Ù„Ø§ØµÙ‡ Û±Û° Ø®ÙˆØ´Ù‡ (Ø¨Ø§ ØªÙ…Ø§Ù… Ø§Ù…Ú©Ø§Ù†Ø§Øª):\")\n",
    "print(summary.sort_values('avg_price_mil', ascending=False))\n",
    "\n",
    "# ---------- 15. ØªØ­Ù„ÛŒÙ„ Ø®ÙˆØ´Ù‡â€ŒÙ‡Ø§ÛŒ Ù„ÙˆÚ©Ø³ ----------\n",
    "luxury_mask = (summary['pool'] > 0.3) | (summary['jacuzzi'] > 0.2)\n",
    "luxury_clusters = summary[luxury_mask].index.tolist()\n",
    "if luxury_clusters:\n",
    "    print(f\"\\nğŸŒŸ Ø®ÙˆØ´Ù‡â€ŒÙ‡Ø§ÛŒ Ù„ÙˆÚ©Ø³ (Ø¯Ø§Ø±Ø§ÛŒ Ø§Ø³ØªØ®Ø±/Ø¬Ú©ÙˆØ²ÛŒ): {luxury_clusters}\")\n",
    "    for c in luxury_clusters:\n",
    "        row = summary.loc[c]\n",
    "        print(f\"  â€¢ C{c}: Ù‚ÛŒÙ…Øª={row['avg_price_mil']:.0f}M, Ø§Ø³ØªØ®Ø±={row['pool']:.0%}, Ø¬Ú©ÙˆØ²ÛŒ={row['jacuzzi']:.0%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "079516d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------\n",
    "# âœ… KMeans Ø¨Ø¯ÙˆÙ† PCA â€” Ø¨Ø§ ØªÙ…Ø§Ù… 11 ÙˆÛŒÚ˜Ú¯ÛŒ Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯ÛŒ Ø´Ù…Ø§\n",
    "#    âœ… ÙÙ‚Ø· Ø¢Ú¯Ù‡ÛŒâ€ŒÙ‡Ø§ÛŒ ÙØ±ÙˆØ´ÛŒ\n",
    "#    âœ… 6 Ø§Ù…Ú©Ø§Ù†Ø§Øª Ø¨Ø§ÛŒÙ†Ø±ÛŒ (Ø§Ø³ØªØ®Ø±ØŒ Ø¬Ú©ÙˆØ²ÛŒØŒ Ø¢Ø³Ø§Ù†Ø³ÙˆØ±ØŒ ...)\n",
    "#    âœ… K=10 Ø¯Ù‚ÛŒÙ‚Ø§Ù‹ Ù…Ø·Ø§Ø¨Ù‚ ØµÙˆØ±Øª Ø³ÙˆØ§Ù„\n",
    "# --------------------------------------------------\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.cluster import KMeans\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "\n",
    "df = data.copy()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df['rooms_num'] = df['rooms_count'].apply(parse_rooms).fillna(2)\n",
    "\n",
    "# ---------- 4. Ù¾Ø§Ú©â€ŒØ³Ø§Ø²ÛŒ Ø³Ø§Ù„ Ø³Ø§Ø®Øª ----------\n",
    "def parse_year(val):\n",
    "    if pd.isna(val): return np.nan\n",
    "    s = str(val).strip()\n",
    "    s = persian_to_english_digits(s)\n",
    "    nums = re.findall(r'1[3-4]\\d{2}', s)\n",
    "    return int(nums[0]) if nums else np.nan\n",
    "\n",
    "df['year'] = df['construction_year'].apply(parse_year)\n",
    "\n",
    "# ---------- 5. ÙÛŒÙ„ØªØ± ÙÙ‚Ø· Ø¢Ú¯Ù‡ÛŒâ€ŒÙ‡Ø§ÛŒ ÙØ±ÙˆØ´ÛŒ ----------\n",
    "df_sell = df[\n",
    "    (df['cat2_slug'].str.contains('sell', case=False, na=False)) &\n",
    "    (df['price_value'].notna()) &\n",
    "    (df['building_size'].notna()) &\n",
    "    (df['location_latitude'].between(25, 40)) &\n",
    "    (df['location_longitude'].between(44, 63))\n",
    "].copy()\n",
    "\n",
    "# ---------- 6. ØªØ¨Ø¯ÛŒÙ„ Ø¹Ø¯Ø¯ÛŒ + ÙÛŒÙ„ØªØ± Ù…Ù†Ø·Ù‚ÛŒ ----------\n",
    "df_sell['price'] = pd.to_numeric(df_sell['price_value'], errors='coerce')\n",
    "df_sell['size'] = pd.to_numeric(df_sell['building_size'], errors='coerce')\n",
    "\n",
    "df_sell = df_sell[\n",
    "    (df_sell['price'] > 10_000_000) & (df_sell['price'] <= 200_000_000_000) &\n",
    "    (df_sell['size'] > 10) & (df_sell['size'] <= 50_000) &\n",
    "    (df_sell['year'].between(1300, 1405))\n",
    "]\n",
    "\n",
    "print(f\"âœ… Ø¢Ú¯Ù‡ÛŒâ€ŒÙ‡Ø§ÛŒ ÙØ±ÙˆØ´ÛŒ Ù…Ø¹ØªØ¨Ø±: {len(df_sell)}\")\n",
    "\n",
    "# ---------- 7. ØªØ¨Ø¯ÛŒÙ„ Ø§Ù…Ú©Ø§Ù†Ø§Øª Ø¨Ù‡ Ø¨Ø§ÛŒÙ†Ø±ÛŒ (0/1) ----------\n",
    "bool_cols = [\n",
    "    'has_pool', 'has_jacuzzi', 'has_elevator',\n",
    "    'has_warehouse', 'has_parking', 'has_security_guard'\n",
    "]\n",
    "\n",
    "for col in bool_cols:\n",
    "    df_sell[col] = df_sell[col].astype(str).str.lower().isin(['true', '1', 'yes']).astype(int)\n",
    "\n",
    "# ---------- 8. Ú©Ø¯Ú¯Ø°Ø§Ø±ÛŒ Ø´Ù‡Ø± ----------\n",
    "le_city = LabelEncoder()\n",
    "city_counts = df_sell['city_slug'].value_counts()\n",
    "valid_cities = city_counts[city_counts >= 10].index\n",
    "df_sell['city_encoded'] = df_sell['city_slug'].where(df_sell['city_slug'].isin(valid_cities), 'Ø³Ø§ÛŒØ±')\n",
    "df_sell['city_encoded'] = le_city.fit_transform(df_sell['city_encoded'])\n",
    "\n",
    "# ---------- 9. ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ù†Ù‡Ø§ÛŒÛŒ (11 Ù…ÙˆØ±Ø¯) ----------\n",
    "features = [\n",
    "    'price', 'size', 'rooms_num', 'year', 'city_encoded',\n",
    "    'has_pool', 'has_jacuzzi', 'has_elevator',\n",
    "    'has_warehouse', 'has_parking', 'has_security_guard'\n",
    "]\n",
    "\n",
    "X = df_sell[features].copy().dropna()\n",
    "print(f\"âœ… Ù¾Ø³ Ø§Ø² Ø­Ø°Ù NaN: {len(X)} Ø±Ú©ÙˆØ±Ø¯\")\n",
    "\n",
    "# ---------- 10. âœ… StandardScaler (Ø­ÛŒØ§ØªÛŒ!) ----------\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# ---------- 11. âœ… KMeans Ø¨Ø§ K=10 (Ø¨Ø¯ÙˆÙ† PCA) ----------\n",
    "kmeans = KMeans(n_clusters=10, random_state=42, n_init=25)\n",
    "clusters = kmeans.fit_predict(X_scaled)\n",
    "df_sell = df_sell.loc[X.index].copy()\n",
    "df_sell['cluster'] = clusters\n",
    "\n",
    "# ---------- 12. Ø±Ø³Ù…: Ù‚ÛŒÙ…Øª vs Ø²ÛŒØ±Ø¨Ù†Ø§ (Ø¨Ø§ 10 Ø®ÙˆØ´Ù‡) ----------\n",
    "df_sell['price_mil'] = df_sell['price'] / 1_000_000\n",
    "\n",
    "fig = px.scatter(\n",
    "    df_sell,\n",
    "    x='size',\n",
    "    y='price_mil',\n",
    "    color='cluster',\n",
    "    color_continuous_scale='Rainbow',\n",
    "    labels={\n",
    "        'size': 'Ø²ÛŒØ±Ø¨Ù†Ø§ (Ù…ØªØ± Ù…Ø±Ø¨Ø¹)',\n",
    "        'price_mil': 'Ù‚ÛŒÙ…Øª ÙØ±ÙˆØ´ (Ù…ÛŒÙ„ÛŒÙˆÙ† ØªÙˆÙ…Ø§Ù†)',\n",
    "        'cluster': 'Ø®ÙˆØ´Ù‡'\n",
    "    },\n",
    "    title='Ø®ÙˆØ´Ù‡â€ŒØ¨Ù†Ø¯ÛŒ KMeans (K=10) â€” Ø¨Ø¯ÙˆÙ† PCAØŒ Ø¨Ø§ 11 ÙˆÛŒÚ˜Ú¯ÛŒ Ù…Ø¹Ù†Ø§Ø¯Ø§Ø±',\n",
    "    opacity=0.65,\n",
    "    width=950, height=650\n",
    ")\n",
    "\n",
    "# Ù…Ø±Ø§Ú©Ø² Ø®ÙˆØ´Ù‡â€ŒÙ‡Ø§ (Ø¯Ø± ÙØ¶Ø§ÛŒ Ø§ØµÙ„ÛŒ)\n",
    "centers_scaled = kmeans.cluster_centers_\n",
    "centers_original = scaler.inverse_transform(centers_scaled)\n",
    "centers_df = pd.DataFrame(centers_original, columns=features)\n",
    "\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=centers_df['size'],\n",
    "    y=centers_df['price'] / 1_000_000,\n",
    "    mode='markers+text',\n",
    "    marker=dict(size=18, color='black', symbol='star', line=dict(width=2, color='white')),\n",
    "    text=[f'C{i}' for i in range(10)],\n",
    "    textposition=\"top center\",\n",
    "    name='Ù…Ø±Ø§Ú©Ø² Ø®ÙˆØ´Ù‡â€ŒÙ‡Ø§'\n",
    "))\n",
    "\n",
    "fig.update_layout(font=dict(size=13), title_font_size=18)\n",
    "fig.write_html(\"kmeans_no_pca_full_features.html\")\n",
    "fig.show()\n",
    "\n",
    "# ---------- 13. Ø®Ù„Ø§ØµÙ‡ Ø®ÙˆØ´Ù‡â€ŒÙ‡Ø§ ----------\n",
    "summary = df_sell.groupby('cluster').agg(\n",
    "    count=('price', 'size'),\n",
    "    avg_price_mil=('price_mil', 'mean'),\n",
    "    avg_size=('size', 'mean'),\n",
    "    avg_rooms=('rooms_num', 'mean'),\n",
    "    avg_year=('year', 'mean'),\n",
    "    main_city=('city_slug', lambda x: x.mode().iloc[0] if not x.mode().empty else 'â€”'),\n",
    "    pool=('has_pool', 'mean'),\n",
    "    jacuzzi=('has_jacuzzi', 'mean'),\n",
    "    elevator=('has_elevator', 'mean'),\n",
    "    warehouse=('has_warehouse', 'mean'),\n",
    "    parking=('has_parking', 'mean'),\n",
    "    guard=('has_security_guard', 'mean')\n",
    ").round(3)\n",
    "\n",
    "print(\"\\nğŸ“Š Ø®Ù„Ø§ØµÙ‡ Û±Û° Ø®ÙˆØ´Ù‡ (Ø¨Ø¯ÙˆÙ† PCA â€” Ø¨Ø§ ØªÙ…Ø§Ù… Ø§Ù…Ú©Ø§Ù†Ø§Øª):\")\n",
    "print(summary.sort_values('avg_price_mil', ascending=False))\n",
    "\n",
    "# ---------- 14. ØªØ­Ù„ÛŒÙ„ Ø®ÙˆØ´Ù‡â€ŒÙ‡Ø§ÛŒ Ù„ÙˆÚ©Ø³ ----------\n",
    "luxury_mask = (summary['pool'] > 0.3) | (summary['jacuzzi'] > 0.2)\n",
    "luxury_clusters = summary[luxury_mask].index.tolist()\n",
    "if luxury_clusters:\n",
    "    print(f\"\\nğŸŒŸ Ø®ÙˆØ´Ù‡â€ŒÙ‡Ø§ÛŒ Ù„ÙˆÚ©Ø³ (Ø¯Ø§Ø±Ø§ÛŒ Ø§Ø³ØªØ®Ø± Ùˆ/ÛŒØ§ Ø¬Ú©ÙˆØ²ÛŒ): {luxury_clusters}\")\n",
    "    for c in luxury_clusters:\n",
    "        row = summary.loc[c]\n",
    "        print(f\"  â€¢ C{c}: Ù‚ÛŒÙ…Øª={row['avg_price_mil']:.0f}M | Ø§Ø³ØªØ®Ø±={row['pool']:.0%} | Ø¬Ú©ÙˆØ²ÛŒ={row['jacuzzi']:.0%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8a200e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------\n",
    "# ğŸ§  Feature Engineering + Feature Selection â†’ KMeans (K=10)\n",
    "# --------------------------------------------------\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.feature_selection import VarianceThreshold, SelectKBest, mutual_info_regression, RFE\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "df = data.copy()\n",
    "\n",
    "\n",
    "# ÙÙ‚Ø· ÙØ±ÙˆØ´ÛŒâ€ŒÙ‡Ø§\n",
    "df = df[df['cat2_slug'].str.contains('sell', case=False, na=False)].copy()\n",
    "\n",
    "# ØªØ¨Ø¯ÛŒÙ„ Ø¹Ø¯Ø¯ÛŒ\n",
    "for col in ['price_value', 'building_size', 'location_latitude', 'location_longitude']:\n",
    "    df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "# ÙÛŒÙ„ØªØ± Ù…Ø¹ØªØ¨Ø±\n",
    "df = df[\n",
    "    (df['price_value'] > 10_000_000) & (df['price_value'] <= 200_000_000_000) &\n",
    "    (df['building_size'] > 10) & (df['building_size'] <= 50_000) &\n",
    "    (df['location_latitude'].between(25, 40)) &\n",
    "    (df['location_longitude'].between(44, 63))\n",
    "]\n",
    "\n",
    "print(f\"âœ… Ø¢Ú¯Ù‡ÛŒâ€ŒÙ‡Ø§ÛŒ ÙØ±ÙˆØ´ÛŒ Ù…Ø¹ØªØ¨Ø±: {len(df)}\")\n",
    "\n",
    "# ---------- 2. Feature Engineering ----------\n",
    "# a) rooms_num\n",
    "def parse_rooms(val):\n",
    "    if pd.isna(val): return np.nan\n",
    "    s = str(val).lower()\n",
    "    if 'Ø¨Ø¯ÙˆÙ†' in s: return 0\n",
    "    if 'ÛŒÚ©' in s: return 1\n",
    "    if 'Ø¯Ùˆ' in s: return 2\n",
    "    if 'Ø³Ù‡' in s: return 3\n",
    "    if 'Ú†Ù‡Ø§Ø±' in s: return 4\n",
    "    if 'Ù¾Ù†Ø¬' in s: return 5\n",
    "    nums = re.findall(r'\\d+', s)\n",
    "    return int(nums[0]) if nums else np.nan\n",
    "\n",
    "df['rooms_num'] = df['rooms_count'].apply(parse_rooms).fillna(2)\n",
    "\n",
    "# b) year\n",
    "def parse_year(val):\n",
    "    if pd.isna(val): return np.nan\n",
    "    s = str(val)\n",
    "    s = persian_to_english_digits(s)\n",
    "    nums = re.findall(r'1[3-4]\\d{2}', s)\n",
    "    return int(nums[0]) if nums else np.nan\n",
    "\n",
    "df['year'] = df['construction_year'].apply(parse_year)\n",
    "\n",
    "# c) Ø§Ù…Ú©Ø§Ù†Ø§Øª Ø¨Ø§ÛŒÙ†Ø±ÛŒ\n",
    "bool_cols = ['has_pool', 'has_jacuzzi', 'has_elevator', \n",
    "             'has_warehouse', 'has_parking', 'has_security_guard']\n",
    "for col in bool_cols:\n",
    "    df[col] = df[col].astype(str).str.lower().isin(['true', '1', 'yes']).astype(int)\n",
    "\n",
    "# d) âœ… ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ù…Ù‡Ù†Ø¯Ø³ÛŒâ€ŒØ´Ø¯Ù‡ (Ú©Ù„ÛŒØ¯ Ù…ÙˆÙÙ‚ÛŒØª!)\n",
    "df['price_per_sqm'] = df['price_value'] / df['building_size']          # Ù‚ÛŒÙ…Øª Ù‡Ø± Ù…ØªØ±\n",
    "df['age'] = 1405 - df['year']                                         # Ø³Ù† Ù…Ù„Ú©\n",
    "df['luxury_score'] = df[['has_pool', 'has_jacuzzi']].sum(axis=1)      # Ø§Ù…ØªÛŒØ§Ø² Ù„ÙˆÚ©Ø³ (0-2)\n",
    "df['basic_score'] = df[['has_elevator', 'has_parking', 'has_security_guard']].sum(axis=1)  # Ø§Ù…ØªÛŒØ§Ø² Ù¾Ø§ÛŒÙ‡ (0-3)\n",
    "\n",
    "# e) Ø´Ù‡Ø± (ÙÙ‚Ø· Ø´Ù‡Ø±Ù‡Ø§ÛŒ Ù…Ù‡Ù…)\n",
    "le_city = LabelEncoder()\n",
    "city_counts = df['city_slug'].value_counts()\n",
    "valid_cities = city_counts[city_counts >= 10].index\n",
    "df['city_encoded'] = df['city_slug'].where(df['city_slug'].isin(valid_cities), 'Ø³Ø§ÛŒØ±')\n",
    "df['city_encoded'] = le_city.fit_transform(df['city_encoded'])\n",
    "\n",
    "# ---------- 3. Ø§Ù†ØªØ®Ø§Ø¨ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø§ÙˆÙ„ÛŒÙ‡ ----------\n",
    "base_features = [\n",
    "    'price_value', 'building_size', 'rooms_num', 'age',\n",
    "    'luxury_score', 'basic_score', 'city_encoded'\n",
    "]\n",
    "\n",
    "X = df[base_features].dropna()\n",
    "y = X['price_value']  # Ø¨Ø±Ø§ÛŒ mutual info\n",
    "X = X.drop(columns=['price_value'])\n",
    "\n",
    "print(f\"âœ… Ù¾Ø³ Ø§Ø² feature engineering: {len(X)} Ø±Ú©ÙˆØ±Ø¯ Ã— {X.shape[1]} ÙˆÛŒÚ˜Ú¯ÛŒ\")\n",
    "\n",
    "# ---------- 4. Feature Selection â€” Ø³Ù‡ Ù…Ø±Ø­Ù„Ù‡ Ù‡ÙˆØ´Ù…Ù†Ø¯Ø§Ù†Ù‡ ----------\n",
    "\n",
    "# Ù…Ø±Ø­Ù„Ù‡ 1: Ø­Ø°Ù ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø¨Ø§ ÙˆØ§Ø±ÛŒØ§Ù†Ø³ Ø¨Ø³ÛŒØ§Ø± Ú©Ù…\n",
    "selector_var = VarianceThreshold(threshold=0.01)\n",
    "X_var = selector_var.fit_transform(X)\n",
    "selected_var = X.columns[selector_var.get_support()]\n",
    "print(f\"Ù¾Ø³ Ø§Ø² VarianceThreshold: {X_var.shape[1]} ÙˆÛŒÚ˜Ú¯ÛŒ\")\n",
    "\n",
    "# Ù…Ø±Ø­Ù„Ù‡ 2: Ø§Ù…ØªÛŒØ§Ø²Ø¯Ù‡ÛŒ Ø¨Ø§ Mutual Information\n",
    "mi_scores = mutual_info_regression(X_var, y[X.index], random_state=42)\n",
    "mi_df = pd.DataFrame({'feature': selected_var, 'mi': mi_scores}).sort_values('mi', ascending=False)\n",
    "print(\"\\nØ§Ù…ØªÛŒØ§Ø² MI (Ø¨Ø§Ù„Ø§ØªØ± = Ù…Ù‡Ù…â€ŒØªØ±):\")\n",
    "print(mi_df)\n",
    "\n",
    "# Ù…Ø±Ø­Ù„Ù‡ 3: Ø§Ù†ØªØ®Ø§Ø¨ Ø¨Ù‡ÛŒÙ†Ù‡ Ø¨Ø§ RFE + Random Forest\n",
    "rf = RandomForestRegressor(n_estimators=50, random_state=42)\n",
    "rfe = RFE(rf, n_features_to_select=1)  # Ø¨Ø±Ø§ÛŒ Ø±ØªØ¨Ù‡â€ŒØ¨Ù†Ø¯ÛŒ\n",
    "rfe.fit(X_var, y[X.index])\n",
    "rfe_ranking = pd.DataFrame({\n",
    "    'feature': selected_var,\n",
    "    'rank': rfe.ranking_\n",
    "}).sort_values('rank')\n",
    "\n",
    "print(\"\\nØ±ØªØ¨Ù‡â€ŒØ¨Ù†Ø¯ÛŒ RFE (Ù¾Ø§ÛŒÛŒÙ†â€ŒØªØ± = Ù…Ù‡Ù…â€ŒØªØ±):\")\n",
    "print(rfe_ranking)\n",
    "\n",
    "# ---------- 5. ØªØ¹ÛŒÛŒÙ† ØªØ¹Ø¯Ø§Ø¯ Ø¨Ù‡ÛŒÙ†Ù‡ ÙˆÛŒÚ˜Ú¯ÛŒ ----------\n",
    "best_k = None\n",
    "best_score = -1\n",
    "scores = []\n",
    "\n",
    "for k in range(2, len(selected_var)+1):\n",
    "    # Ø§Ù†ØªØ®Ø§Ø¨ k ÙˆÛŒÚ˜Ú¯ÛŒ Ø¨Ø±ØªØ± (ØªØ±Ú©ÛŒØ¨ MI + RFE)\n",
    "    top_k = mi_df.head(k)['feature'].tolist()\n",
    "    X_k = X[top_k]\n",
    "    \n",
    "    # scaling\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X_k)\n",
    "    \n",
    "    # KMeans Ø¨Ø§ K=10\n",
    "    kmeans = KMeans(n_clusters=10, random_state=42, n_init=10)\n",
    "    labels = kmeans.fit_predict(X_scaled)\n",
    "    \n",
    "    # Silhouette Score\n",
    "    sil = silhouette_score(X_scaled, labels, sample_size=min(5000, len(X_scaled)))\n",
    "    scores.append((k, sil))\n",
    "    \n",
    "    if sil > best_score:\n",
    "        best_score = sil\n",
    "        best_k = k\n",
    "\n",
    "print(f\"\\nâœ… Ø¨Ù‡ØªØ±ÛŒÙ† ØªØ¹Ø¯Ø§Ø¯ ÙˆÛŒÚ˜Ú¯ÛŒ: {best_k} â†’ Silhouette = {best_score:.4f}\")\n",
    "\n",
    "# Ø±Ø³Ù…\n",
    "k_vals, sil_vals = zip(*scores)\n",
    "fig = px.line(x=k_vals, y=sil_vals, markers=True,\n",
    "              labels={'x': 'ØªØ¹Ø¯Ø§Ø¯ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§', 'y': 'Silhouette Score'},\n",
    "              title='ØªØ¹ÛŒÛŒÙ† ØªØ¹Ø¯Ø§Ø¯ Ø¨Ù‡ÛŒÙ†Ù‡ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§')\n",
    "fig.add_vline(x=best_k, line_dash=\"dash\", line_color=\"red\", \n",
    "              annotation_text=f\"Ø¨Ù‡ÛŒÙ†Ù‡: K={best_k}\", annotation_position=\"top right\")\n",
    "fig.show()\n",
    "\n",
    "# ---------- 6. Ø§Ù†ØªØ®Ø§Ø¨ Ù†Ù‡Ø§ÛŒÛŒ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ ----------\n",
    "final_features = mi_df.head(best_k)['feature'].tolist()\n",
    "print(f\"\\nâœ… ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ù†Ù‡Ø§ÛŒÛŒ ({len(final_features)} Ù…ÙˆØ±Ø¯):\")\n",
    "print(final_features)\n",
    "\n",
    "X_final = X[final_features].copy()\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_final)\n",
    "\n",
    "# ---------- 7. âœ… KMeans Ø¨Ø§ K=10 ----------\n",
    "kmeans = KMeans(n_clusters=10, random_state=42, n_init=25)\n",
    "clusters = kmeans.fit_predict(X_scaled)\n",
    "df_final = df.loc[X_final.index].copy()\n",
    "df_final['cluster'] = clusters\n",
    "\n",
    "# ---------- 8. Ø±Ø³Ù… Ù†ØªÛŒØ¬Ù‡ ----------\n",
    "df_final['price_mil'] = df_final['price_value'] / 1_000_000\n",
    "\n",
    "fig = px.scatter(\n",
    "    df_final,\n",
    "    x='building_size',\n",
    "    y='price_mil',\n",
    "    color='cluster',\n",
    "    color_continuous_scale='Rainbow',\n",
    "    labels={\n",
    "        'building_size': 'Ø²ÛŒØ±Ø¨Ù†Ø§ (Ù…ØªØ± Ù…Ø±Ø¨Ø¹)',\n",
    "        'price_mil': 'Ù‚ÛŒÙ…Øª (Ù…ÛŒÙ„ÛŒÙˆÙ† ØªÙˆÙ…Ø§Ù†)',\n",
    "        'cluster': 'Ø®ÙˆØ´Ù‡'\n",
    "    },\n",
    "    title=f'Ø®ÙˆØ´Ù‡â€ŒØ¨Ù†Ø¯ÛŒ Ù†Ù‡Ø§ÛŒÛŒ (K=10) â€” Ø¨Ø§ {best_k} ÙˆÛŒÚ˜Ú¯ÛŒ Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ´Ø¯Ù‡',\n",
    "    opacity=0.65,\n",
    "    width=950, height=650\n",
    ")\n",
    "\n",
    "# Ù…Ø±Ø§Ú©Ø²\n",
    "centers = scaler.inverse_transform(kmeans.cluster_centers_)\n",
    "centers_df = pd.DataFrame(centers, columns=final_features)\n",
    "if 'building_size' in centers_df.columns and 'price_value' in df_final.columns:\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=centers_df['building_size'] if 'building_size' in centers_df.columns else [0]*10,\n",
    "        y=[c / 1e6 for c in kmeans.cluster_centers_[:, final_features.index('building_size')] \n",
    "           if 'building_size' in final_features else [0]*10],\n",
    "        mode='markers+text',\n",
    "        marker=dict(size=18, color='black', symbol='star'),\n",
    "        text=[f'C{i}' for i in range(10)],\n",
    "        textposition=\"top center\",\n",
    "        showlegend=False\n",
    "    ))\n",
    "\n",
    "fig.update_layout(font=dict(size=13))\n",
    "fig.write_html(\"kmeans_optimized_features.html\")\n",
    "fig.show()\n",
    "\n",
    "# ---------- 9. Ø®Ù„Ø§ØµÙ‡ Ø®ÙˆØ´Ù‡â€ŒÙ‡Ø§ ----------\n",
    "summary = df_final.groupby('cluster').agg(\n",
    "    count=('price_value', 'size'),\n",
    "    avg_price_mil=('price_mil', 'mean'),\n",
    "    avg_size=('building_size', 'mean'),\n",
    "    avg_age=('age', 'mean'),\n",
    "    avg_luxury=('luxury_score', 'mean'),\n",
    "    avg_basic=('basic_score', 'mean'),\n",
    "    main_city=('city_slug', lambda x: x.mode().iloc[0] if not x.mode().empty else 'â€”')\n",
    ").round(2)\n",
    "\n",
    "print(\"\\nğŸ“Š Ø®Ù„Ø§ØµÙ‡ Û±Û° Ø®ÙˆØ´Ù‡ (Ø¨Ø§ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ´Ø¯Ù‡):\")\n",
    "print(summary.sort_values('avg_price_mil', ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4110394",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------\n",
    "# ğŸ§  Auto Feature Selection + KMeans (K=10)\n",
    "#    âœ… Ù‡Ù…Ù‡ Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ ÙˆØ±ÙˆØ¯ÛŒ Ù‡Ø³ØªÙ†Ø¯\n",
    "#    âœ… Ø¨Ø¯ÙˆÙ† ÙØ±Ø¶ Ø§ÙˆÙ„ÛŒÙ‡\n",
    "#    âœ… Imputation ØµØ­ÛŒØ­ (Ø¨Ø¯ÙˆÙ† Shape mismatch)\n",
    "# --------------------------------------------------\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.feature_selection import VarianceThreshold, SelectKBest, f_regression, RFE\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "import plotly.express as px\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "df = data.copy()\n",
    "print(f\"âœ… Ø¯Ø§Ø¯Ù‡ Ø§ÙˆÙ„ÛŒÙ‡: {df.shape[0]:,} Ø±Ø¯ÛŒÙ Ã— {df.shape[1]} Ø³ØªÙˆÙ†\")\n",
    "\n",
    "\n",
    "\n",
    "# ---------- 2. Ù¾Ø§Ú©â€ŒØ³Ø§Ø²ÛŒ Ø§ÙˆÙ„ÛŒÙ‡ ----------\n",
    "df = df.dropna(thresh=int(0.05 * len(df)), axis=1)  # Ø­Ø°Ù Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ >95% NaN\n",
    "nunique = df.nunique()\n",
    "df = df.drop(columns=nunique[nunique == 1].index)  # Ø­Ø°Ù Ø«Ø§Ø¨Øªâ€ŒÙ‡Ø§\n",
    "print(f\"Ù¾Ø³ Ø§Ø² Ù¾Ø§Ú©â€ŒØ³Ø§Ø²ÛŒ Ø§ÙˆÙ„ÛŒÙ‡: {df.shape[1]} Ø³ØªÙˆÙ†\")\n",
    "\n",
    "# ---------- 3. ØªØ¨Ø¯ÛŒÙ„ Ø¨Ù‡ Ø§Ù†ÙˆØ§Ø¹ Ù…Ù†Ø§Ø³Ø¨ ----------\n",
    "# a) Ø¹Ø¯Ø¯ÛŒ\n",
    "numeric_cols = []\n",
    "for col in df.columns:\n",
    "    try:\n",
    "        pd.to_numeric(df[col], errors='raise')\n",
    "        numeric_cols.append(col)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "df_num = df[numeric_cols].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# b) Ø¨Ø§ÛŒÙ†Ø±ÛŒ\n",
    "binary_cols = []\n",
    "for col in df.columns:\n",
    "    if col not in numeric_cols:\n",
    "        vals = set(str(x).strip().lower() for x in df[col].dropna().unique())\n",
    "        if vals <= {'true', 'false', '0', '1', 'yes', 'no', 'unselect'}:\n",
    "            binary_cols.append(col)\n",
    "\n",
    "df_bin = pd.DataFrame()\n",
    "for col in binary_cols:\n",
    "    df_bin[col + '_bin'] = df[col].astype(str).str.lower().isin(['true', '1', 'yes']).astype(int)\n",
    "\n",
    "# c) Ø¯Ø³ØªÙ‡â€ŒØ§ÛŒ (ÙÙ‚Ø· Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ Ù…ÙˆØ¬ÙˆØ¯ Ùˆ Ø¨Ø§ Ú©Ø§Ø±Ø¯ÛŒÙ†Ø§Ù„ÛŒØªÙ‡ Ù…Ø¹Ù‚ÙˆÙ„)\n",
    "cat_cols_candidate = ['city_slug', 'cat2_slug', 'cat3_slug', 'neighborhood_slug', 'deed_type']\n",
    "cat_cols = [col for col in cat_cols_candidate if col in df.columns and 2 <= df[col].nunique() <= 50]\n",
    "\n",
    "df_cat = pd.DataFrame(index=df.index)\n",
    "for col in cat_cols:\n",
    "    le = LabelEncoder()\n",
    "    series = df[col].fillna('Ù†Ø§Ø´Ù†Ø§Ø³').astype(str)\n",
    "    df_cat[col + '_enc'] = le.fit_transform(series)\n",
    "\n",
    "# d) Feature Engineering (Ø®ÙˆØ¯Ú©Ø§Ø±)\n",
    "engineered = pd.DataFrame(index=df.index)\n",
    "if 'price_value' in df_num.columns and 'building_size' in df_num.columns:\n",
    "    p = df_num['price_value']\n",
    "    s = df_num['building_size']\n",
    "    engineered['price_per_sqm'] = p / s\n",
    "    engineered['log_price'] = np.log1p(p)\n",
    "    engineered['log_size'] = np.log1p(s)\n",
    "\n",
    "# ---------- 4. ØªØ±Ú©ÛŒØ¨ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ ----------\n",
    "X_raw = pd.concat([df_num, df_bin, df_cat, engineered], axis=1)\n",
    "print(f\"âœ… Ù¾Ø³ Ø§Ø² Ù…Ù‡Ù†Ø¯Ø³ÛŒ: {X_raw.shape[1]} ÙˆÛŒÚ˜Ú¯ÛŒ\")\n",
    "\n",
    "# Ø­Ø°Ù Ù‡Ù…Ø¨Ø³ØªÚ¯ÛŒ Ø¨Ø§Ù„Ø§\n",
    "corr = X_raw.corr().abs()\n",
    "upper = corr.where(np.triu(np.ones(corr.shape), k=1).astype(bool))\n",
    "to_drop = [c for c in upper.columns if any(upper[c] > 0.95)]\n",
    "X_raw = X_raw.drop(columns=to_drop)\n",
    "print(f\"Ù¾Ø³ Ø§Ø² Ø­Ø°Ù Ù‡Ù…Ø¨Ø³ØªÚ¯ÛŒ Ø¨Ø§Ù„Ø§: {X_raw.shape[1]} ÙˆÛŒÚ˜Ú¯ÛŒ\")\n",
    "\n",
    "# ---------- 5. Imputation ØµØ­ÛŒØ­ (Ø¨Ø¯ÙˆÙ† Shape mismatch) ----------\n",
    "# Ø¬Ø¯Ø§ Ú©Ø±Ø¯Ù† y (Ø§ÙˆÙ„ÙˆÛŒØª: price_value)\n",
    "# ---------- Imputation â€” Ø±ÙˆØ´ Ù‚Ø·Ø¹ÛŒ (Ø¨Ø¯ÙˆÙ† Shape mismatch) ----------\n",
    "# Ø¬Ø¯Ø§ Ú©Ø±Ø¯Ù† y Ùˆ Ù‡Ù…Ø§Ù‡Ù†Ú¯ÛŒ Ø§ÛŒÙ†Ø¯Ú©Ø³\n",
    "y = None\n",
    "X_temp = X_raw.copy()\n",
    "\n",
    "for col in ['price_value', 'transformable_price']:\n",
    "    if col in X_temp.columns:\n",
    "        # ØªØ¨Ø¯ÛŒÙ„ Ø¨Ù‡ Ø¹Ø¯Ø¯ÛŒ Ùˆ Ù¾ÛŒØ¯Ø§ Ú©Ø±Ø¯Ù† Ù…Ù‚Ø§Ø¯ÛŒØ± Ù…Ø¹ØªØ¨Ø±\n",
    "        y_series = pd.to_numeric(X_temp[col], errors='coerce')\n",
    "        mask = y_series.notna()\n",
    "        y = y_series[mask]\n",
    "        X_temp = X_temp.loc[mask].drop(columns=[col])\n",
    "        print(f\"ğŸ¯ Ù‡Ø¯Ù: '{col}' â†’ {len(y):,} Ù†Ù…ÙˆÙ†Ù‡\")\n",
    "        break\n",
    "\n",
    "if y is None:\n",
    "    # fallback: Ø§ÙˆÙ„ÛŒÙ† Ø³ØªÙˆÙ† Ø¹Ø¯Ø¯ÛŒ\n",
    "    for col in X_temp.columns:\n",
    "        y_series = pd.to_numeric(X_temp[col], errors='coerce')\n",
    "        mask = y_series.notna()\n",
    "        if mask.sum() > 100:  # Ø­Ø¯Ø§Ù‚Ù„ 100 Ù†Ù…ÙˆÙ†Ù‡\n",
    "            y = y_series[mask]\n",
    "            X_temp = X_temp.loc[mask].drop(columns=[col])\n",
    "            print(f\"âš ï¸ Ù‡Ø¯Ù Ù¾ÛŒØ´â€ŒÙØ±Ø¶: '{col}'\")\n",
    "            break\n",
    "\n",
    "# âœ… Ø±ÙˆØ´ Ù‚Ø·Ø¹ÛŒ: Imputation + ÙÛŒÙ„ØªØ± Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ Ù…ÙˆØ¬ÙˆØ¯\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "X_imputed_arr = imputer.fit_transform(X_temp)\n",
    "\n",
    "# ğŸ”‘ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒÛŒ Ú©Ù‡ ÙˆØ§Ù‚Ø¹Ø§Ù‹ Ø¯Ø± Ø®Ø±ÙˆØ¬ÛŒ Ù‡Ø³ØªÙ†Ø¯\n",
    "mask_cols = ~np.isnan(imputer.statistics_)  # Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒÛŒ Ú©Ù‡ imputer Ø±ÙˆÛŒ Ø¢Ù†â€ŒÙ‡Ø§ Ú©Ø§Ø± Ú©Ø±Ø¯Ù‡\n",
    "selected_cols = X_temp.columns[mask_cols]\n",
    "\n",
    "X_imputed = pd.DataFrame(\n",
    "    X_imputed_arr,\n",
    "    columns=selected_cols,  # âœ… ÙÙ‚Ø· Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ Ø¨Ø§Ù‚ÛŒâ€ŒÙ…Ø§Ù†Ø¯Ù‡\n",
    "    index=X_temp.index\n",
    ")\n",
    "\n",
    "print(f\"Ù¾Ø³ Ø§Ø² Imputation: {X_imputed.shape[1]} ÙˆÛŒÚ˜Ú¯ÛŒ (Ø§Ø² {X_temp.shape[1]})\")\n",
    "# ---------- 6. Feature Selection Ø®ÙˆØ¯Ú©Ø§Ø± ----------\n",
    "# Stage 1: VarianceThreshold\n",
    "selector_var = VarianceThreshold(threshold=0.01)\n",
    "X_var = pd.DataFrame(\n",
    "    selector_var.fit_transform(X_imputed),\n",
    "    columns=X_imputed.columns[selector_var.get_support()],\n",
    "    index=X_imputed.index\n",
    ")\n",
    "\n",
    "# Stage 2: SelectKBest\n",
    "if X_var.shape[1] > 0:\n",
    "    k_best = min(20, X_var.shape[1])\n",
    "    selector_k = SelectKBest(score_func=f_regression, k=k_best)\n",
    "    X_kbest = pd.DataFrame(\n",
    "        selector_k.fit_transform(X_var, y),\n",
    "        columns=X_var.columns[selector_k.get_support()],\n",
    "        index=X_var.index\n",
    "    )\n",
    "    print(f\"Ù¾Ø³ Ø§Ø² SelectKBest: {X_kbest.shape[1]} ÙˆÛŒÚ˜Ú¯ÛŒ\")\n",
    "else:\n",
    "    X_kbest = X_var.copy()\n",
    "\n",
    "# Stage 3: RFE Ø¨Ø±Ø§ÛŒ Ø±ØªØ¨Ù‡â€ŒØ¨Ù†Ø¯ÛŒ\n",
    "if not X_kbest.empty:\n",
    "    rf = RandomForestRegressor(n_estimators=50, random_state=42)\n",
    "    rfe = RFE(rf, n_features_to_select=min(10, len(X_kbest.columns)), step=1)\n",
    "    rfe.fit(X_kbest, y)\n",
    "    ranking = pd.DataFrame({\n",
    "        'feature': X_kbest.columns,\n",
    "        'rank': rfe.ranking_\n",
    "    }).sort_values('rank')\n",
    "else:\n",
    "    ranking = pd.DataFrame(columns=['feature', 'rank'])\n",
    "\n",
    "# ---------- 7. ØªØ¹ÛŒÛŒÙ† ØªØ¹Ø¯Ø§Ø¯ Ø¨Ù‡ÛŒÙ†Ù‡ ÙˆÛŒÚ˜Ú¯ÛŒ ----------\n",
    "best_k, best_score = 1, -1\n",
    "scores = []\n",
    "\n",
    "for k in range(1, min(11, len(ranking)+1)):\n",
    "    top_k = ranking.head(k)['feature'].tolist()\n",
    "    if not top_k: break\n",
    "    X_k = X_kbest[top_k]\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X_k)\n",
    "    \n",
    "    kmeans = KMeans(n_clusters=10, random_state=42, n_init=10)\n",
    "    labels = kmeans.fit_predict(X_scaled)\n",
    "    sil = silhouette_score(X_scaled, labels, sample_size=min(5000, len(X_scaled)), random_state=42)\n",
    "    scores.append((k, sil))\n",
    "    if sil > best_score:\n",
    "        best_score, best_k = sil, k\n",
    "\n",
    "print(f\"\\nâœ… Ø¨Ù‡ØªØ±ÛŒÙ† ØªØ¹Ø¯Ø§Ø¯ ÙˆÛŒÚ˜Ú¯ÛŒ: {best_k} â†’ Silhouette = {best_score:.4f}\")\n",
    "\n",
    "# Ø±Ø³Ù…\n",
    "if scores:\n",
    "    k_vals, sil_vals = zip(*scores)\n",
    "    fig = px.line(x=k_vals, y=sil_vals, markers=True,\n",
    "                  labels={'x': 'ØªØ¹Ø¯Ø§Ø¯ ÙˆÛŒÚ˜Ú¯ÛŒ', 'y': 'Silhouette'},\n",
    "                  title='Ø§Ù†ØªØ®Ø§Ø¨ ØªØ¹Ø¯Ø§Ø¯ Ø¨Ù‡ÛŒÙ†Ù‡ ÙˆÛŒÚ˜Ú¯ÛŒ')\n",
    "    fig.add_vline(x=best_k, line_dash=\"dash\", line_color=\"red\")\n",
    "    fig.show()\n",
    "\n",
    "# ---------- 8. KMeans Ù†Ù‡Ø§ÛŒÛŒ (K=10) ----------\n",
    "final_features = ranking.head(best_k)['feature'].tolist() if not ranking.empty else X_kbest.columns[:best_k].tolist()\n",
    "print(f\"\\nâœ… ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ù†Ù‡Ø§ÛŒÛŒ ({len(final_features)}):\")\n",
    "for i, f in enumerate(final_features, 1):\n",
    "    print(f\"{i}. {f}\")\n",
    "\n",
    "X_final = X_kbest[final_features].copy()\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_final)\n",
    "\n",
    "kmeans = KMeans(n_clusters=10, random_state=42, n_init=25)\n",
    "clusters = kmeans.fit_predict(X_scaled)\n",
    "\n",
    "# ---------- 9. Ø®Ø±ÙˆØ¬ÛŒ ----------\n",
    "df_out = df.loc[X_final.index].copy()\n",
    "df_out['cluster'] = clusters\n",
    "\n",
    "summary = df_out.groupby('cluster').agg(\n",
    "    count=('cluster', 'size'),\n",
    "    **{f: (f, 'mean') for f in final_features if f in df_out.columns}\n",
    ").round(2)\n",
    "\n",
    "print(\"\\nğŸ“Š Ø®Ù„Ø§ØµÙ‡ Û±Û° Ø®ÙˆØ´Ù‡:\")\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b5a9317",
   "metadata": {},
   "source": [
    "# Part 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8fbe7ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.impute import SimpleImputer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4800e907",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca353395",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['rooms_num'] = df['rooms_count'].apply(parse_rooms).fillna(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e8c587",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\n",
    "    \"price_value\",\n",
    "    \"building_size\",\n",
    "    \"rooms_num\",\n",
    "    \"construction_year\",\n",
    "    \"location_latitude\",\n",
    "    \"location_longitude\",\n",
    "    \"city_slug\"\n",
    "]\n",
    "\n",
    "df_model = df[features].copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c42ad16",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e976bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model = df_model.dropna(\n",
    "    subset=[\"location_latitude\",\"location_longitude\",\"city_slug\"]\n",
    ").reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f6ec9bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b41b01cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cols = [\n",
    "    \"price_value\",\n",
    "    \"building_size\",\n",
    "    \"rooms_num\",\n",
    "    \"construction_year\"\n",
    "]\n",
    "\n",
    "geo_features=[\n",
    "    \"location_latitude\",\n",
    "    \"location_longitude\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db550bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in num_cols:\n",
    "    df_model[col] = pd.to_numeric(df_model[col], errors=\"coerce\")\n",
    "\n",
    "df_model = df_model.dropna()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db600a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in geo_features:\n",
    "    df_model[col] = pd.to_numeric(df_model[col], errors=\"coerce\")\n",
    "\n",
    "df_model = df_model.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d83b4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model_sell = df_model[\n",
    "    (df_model['price_value']>0) &\n",
    "    (df_model['building_size'].notna()) &\n",
    "    (df_model['location_latitude'].between(25, 40)) &\n",
    "    (df_model['location_longitude'].between(44, 63))\n",
    "].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e58f03a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model_sell.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a883b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "city_mean_price = (\n",
    "    df_model_sell.groupby(\"city_slug\")[\"price_value\"]\n",
    "            .mean()\n",
    ")\n",
    "\n",
    "df_model_sell[\"city_encoded\"] = df_model_sell[\"city_slug\"].map(city_mean_price)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a5dd5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model_sell.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac87a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "imputer = SimpleImputer(strategy=\"median\")\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_num = imputer.fit_transform(df_model_sell[num_cols])\n",
    "X_num = scaler.fit_transform(X_num)\n",
    "\n",
    "\n",
    "# X_geo = imputer.fit_transform(df_model_sell[geo_features])\n",
    "X_geo = scaler.fit_transform(df_model_sell[geo_features])\n",
    "\n",
    "\n",
    "city_encoded = imputer.fit_transform(\n",
    "    df_model_sell[[\"city_encoded\"]]\n",
    ")\n",
    "city_encoded = scaler.fit_transform(city_encoded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d276a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_geo = scaler.fit_transform(geo_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "215a83e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = np.array([3, 1.5, 1, 1])  \n",
    "X_weighted = X_num * weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55527ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "city_weight = 0.3\n",
    "\n",
    "X_final = np.hstack([\n",
    "    X_num,\n",
    "    X_geo,\n",
    "    city_encoded * city_weight\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31555315",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(\n",
    "    n_clusters=10,\n",
    "    random_state=42,\n",
    "    n_init=10\n",
    ")\n",
    "\n",
    "df_model_sell[\"cluster\"] = kmeans.fit_predict(X_final)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e6573f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "R = 6378137  # Ø´Ø¹Ø§Ø¹ Ø²Ù…ÛŒÙ† (Ù…ØªØ±)\n",
    "\n",
    "lat_rad = np.deg2rad(df_model_sell[\"location_latitude\"])\n",
    "lon_rad = np.deg2rad(df_model_sell[\"location_longitude\"])\n",
    "\n",
    "lat_mean = lat_rad.mean()\n",
    "\n",
    "df_model_sell[\"utm_x\"] = R * lon_rad * np.cos(lat_mean)\n",
    "df_model_sell[\"utm_y\"] = R * lat_rad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d5309b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# centers_scaled = kmeans.cluster_centers_\n",
    "# centers = scaler.inverse_transform(centers_scaled)\n",
    "\n",
    "# center_lat = np.deg2rad(centers[:, features.index(\"location_latitude\")])\n",
    "# center_lon = np.deg2rad(centers[:, features.index(\"location_longitude\")])\n",
    "\n",
    "# center_x = R * center_lon * np.cos(lat_mean)\n",
    "# center_price = centers[:, features.index(\"price_value\")]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d570f596",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(11, 7))\n",
    "\n",
    "# scatter = plt.scatter(\n",
    "#     data[\"utm_x\"],\n",
    "#     data[\"price_value\"],\n",
    "#     c=data[\"cluster\"],\n",
    "#     alpha=0.7,\n",
    "#     marker=\"o\"\n",
    "# )\n",
    "\n",
    "# # Ù…Ø±Ø§Ú©Ø² Ø®ÙˆØ´Ù‡â€ŒÙ‡Ø§\n",
    "# plt.scatter(\n",
    "#     center_x,\n",
    "#     center_price,\n",
    "#     marker=\"X\",\n",
    "#     s=250\n",
    "# )\n",
    "\n",
    "# plt.xlabel(\"UTM X Coordinate (meters)\")\n",
    "# plt.ylabel(\"Transformed Price\")\n",
    "# plt.title(\"K-Means Clustering of Properties (k=10)\")\n",
    "\n",
    "# plt.colorbar(scatter, label=\"Cluster ID\")\n",
    "# plt.grid(True)\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e8ed026",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(\n",
    "    df_model_sell[\"utm_x\"],\n",
    "    df_model_sell[\"utm_y\"],\n",
    "    c=df_model_sell[\"cluster\"],\n",
    "    alpha=0.7\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e8f0c7f",
   "metadata": {},
   "source": [
    "# Middle part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b42ea7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(\n",
    "    n_clusters=7,\n",
    "    random_state=42,\n",
    "    n_init=10\n",
    ")\n",
    "\n",
    "df_model_sell[\"cluster\"] = kmeans.fit_predict(X_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d671e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "R = 6378137  # Ø´Ø¹Ø§Ø¹ Ø²Ù…ÛŒÙ† (Ù…ØªØ±)\n",
    "\n",
    "lat_rad = np.deg2rad(df_model_sell[\"location_latitude\"])\n",
    "lon_rad = np.deg2rad(df_model_sell[\"location_longitude\"])\n",
    "\n",
    "lat_mean = lat_rad.mean()\n",
    "\n",
    "df_model_sell[\"utm_x\"] = R * lon_rad * np.cos(lat_mean)\n",
    "df_model_sell[\"utm_y\"] = R * lat_rad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e224d7d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = np.sort(df_model_sell[\"cluster\"].unique())\n",
    "for c in clusters:\n",
    "    cluster_data = df_model_sell[df_model_sell[\"cluster\"] == c]\n",
    "\n",
    "    plt.scatter(\n",
    "        df_model_sell[\"utm_x\"],\n",
    "        df_model_sell[\"utm_y\"],\n",
    "        c=df_model_sell[\"cluster\"],\n",
    "        alpha=0.7,\n",
    "        cmap='tab20',\n",
    "        label=f\"Cluster {c}\"\n",
    "    )\n",
    "\n",
    "plt.legend(\n",
    "    title=\"Clusters\",\n",
    "    markerscale=2,\n",
    "    fontsize=9,\n",
    "    title_fontsize=10,\n",
    "    loc=\"best\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd94afa5",
   "metadata": {},
   "source": [
    "# partIII\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd62abe",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9855dbc9",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac01a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyproj\n",
    "import numpy as np\n",
    "from sklearn.cluster import DBSCAN\n",
    "df=data.copy()\n",
    "df = df.dropna(\n",
    "    subset=[\"location_latitude\",\"location_longitude\",\"transformable_price\"]\n",
    ").reset_index(drop=True)\n",
    "# Ù¾Ø±ÙˆØ¬Ú©Ø´Ù† WGS84 -> UTM Zone 39N (Ù…Ù†Ø§Ø³Ø¨ Ø§ÛŒØ±Ø§Ù†)\n",
    "proj_utm = pyproj.Proj(proj=\"utm\", zone=39, ellps=\"WGS84\")\n",
    "\n",
    "utm_x, utm_y = proj_utm(\n",
    "    df[\"location_longitude\"].values,\n",
    "    df[\"location_latitude\"].values\n",
    ")\n",
    "\n",
    "df[\"utm_x\"] = utm_x\n",
    "df[\"utm_y\"] = utm_y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17137810",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample = df.sample(\n",
    "    n=10000,        # ÛŒØ§ 30000 Ø¨Ø³ØªÙ‡ Ø¨Ù‡ RAM\n",
    "    random_state=42\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc6e7bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_db = df_sample[[\"utm_x\", \"utm_y\", \"transformable_price\"]]\n",
    "\n",
    "X_db_scaled = StandardScaler().fit_transform(X_db)\n",
    "\n",
    "dbscan = DBSCAN(\n",
    "    eps=0.6,\n",
    "    min_samples=50,\n",
    "    algorithm=\"ball_tree\"   # Ù…Ù‡Ù…\n",
    ")\n",
    "\n",
    "df_sample[\"dbscan_cluster\"] = dbscan.fit_predict(X_db_scaled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcfe396c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "for c in sorted(df_sample[\"dbscan_cluster\"].unique()):\n",
    "    subset = df_sample[df_sample[\"dbscan_cluster\"] == c]\n",
    "    label = \"Noise\" if c == -1 else f\"Cluster {c}\"\n",
    "    \n",
    "    plt.scatter(\n",
    "        subset[\"utm_x\"],\n",
    "        subset[\"utm_y\"],\n",
    "        s=12,\n",
    "        alpha=0.6,\n",
    "        label=label\n",
    "    )\n",
    "\n",
    "plt.xlabel(\"UTM X\")\n",
    "plt.ylabel(\"UTM Y\")\n",
    "plt.title(\"DBSCAN Clustering (Sampled Data)\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d2d3f31",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
